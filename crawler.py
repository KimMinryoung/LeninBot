import requests
from bs4 import BeautifulSoup
import os
import time
import re
from urllib.parse import urljoin

# 1. ì„¤ì •
# ê°€ì¥ ë°©ëŒ€í•œ ëª©ë¡ì´ ìˆëŠ” ì—°ë„ë³„ ìƒ‰ì¸ í˜ì´ì§€ë¥¼ ì‹œì‘ì ìœ¼ë¡œ ì¡ìŠµë‹ˆë‹¤.
START_URL = "https://www.marxists.org/archive/lenin/works/index.htm"
OUTPUT_DIR = "./docs/lenin"
DELAY = 0.3 
MAX_DEPTH = 3 # ëª©ì°¨ -> í•˜ìœ„ ëª©ì°¨ -> ì±•í„°ê¹Œì§€ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë„ë¡ ì„¤ì •

if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

visited_urls = set()
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

def get_soup(url):
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        res.encoding = res.apparent_encoding
        return BeautifulSoup(res.text, 'html.parser')
    except Exception as e:
        print(f"âŒ ì ‘ì† ì‹¤íŒ¨: {url} ({e})")
        return None

def is_content_page(url, soup):
    """ì‹¤ì œ ë³¸ë¬¸ì´ ë“¤ì–´ìˆëŠ” í˜ì´ì§€ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    # 190x/xx/xx.htm ì‹ì˜ íŒ¨í„´ì´ê±°ë‚˜ í…ìŠ¤íŠ¸ ì–‘ì´ ë§ìœ¼ë©´ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
    text_content = soup.get_text()
    return len(text_content) > 1500 and not url.endswith('index.htm')

def crawl(url, depth):
    if depth > MAX_DEPTH or url in visited_urls:
        return
    
    visited_urls.add(url)
    soup = get_soup(url)
    if not soup: return

    # 1. ë§Œì•½ í˜„ì¬ í˜ì´ì§€ê°€ ë³¸ë¬¸ì´ë¼ë©´ ì €ì¥
    if is_content_page(url, soup):
        save_document(url, soup)
        return # ë³¸ë¬¸ í˜ì´ì§€ ì•ˆì˜ ë§í¬ëŠ” ë” ì´ìƒ íƒ€ì§€ ì•ŠìŒ

    # 2. ëª©ì°¨ í˜ì´ì§€ë¼ë©´ í•˜ìœ„ ë§í¬ ìˆ˜ì§‘ ë° ì¬ê·€ íƒìƒ‰
    print(f"ğŸ“‚ íƒìƒ‰ ì¤‘ (Depth {depth}): {url}")
    links = soup.find_all('a', href=True)
    
    for a in links:
        href = a['href']
        full_url = urljoin(url, href).split('#')[0] # ì•µì»¤ ì œê±°

        # ë ˆë‹Œì˜ ì €ì‘ ê²½ë¡œ(works) ë‚´ë¶€ì— ìˆê³ , htm í™•ì¥ìì´ë©°, ì´ë¯¸ ë°©ë¬¸í•˜ì§€ ì•Šì€ ê²½ìš°ë§Œ
        if "marxists.org/archive/lenin/works/" in full_url:
            if full_url.endswith('.htm') or full_url.endswith('.html') or full_url.endswith('/'):
                crawl(full_url, depth + 1)

def save_document(url, soup):
    # ë¬¸í—Œ ì œëª© ì¶”ì¶œ (nav ì œê±° ì „ì— ìˆ˜í–‰)
    title = soup.title.string.strip() if soup.title and soup.title.string else ""

    # ë‚´ë¹„ê²Œì´ì…˜ ìš”ì†Œ ì œê±°
    for nav in soup.find_all(['nav', 'header', 'footer', 'table']):
        nav.decompose()

    # ë³¸ë¬¸ ì¶”ì¶œ
    paragraphs = soup.find_all(['p', 'h3', 'h4', 'blockquote'])
    content = "\n\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text()) > 30])

    if len(content) < 500: return # ë„ˆë¬´ ì§§ì€ ë°ì´í„°ëŠ” ë²„ë¦¼

    # íŒŒì¼ëª… ìƒì„± (URL êµ¬ì¡° ë°˜ì˜)
    path_part = url.split('works/')[-1]
    file_name = re.sub(r'[/\\?%*:|"<>]', '_', path_part).replace('.htm', '.txt').replace('.html', '.txt')

    save_path = os.path.join(OUTPUT_DIR, file_name)

    try:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(f"Source: {url}\nTitle: {title}\n\n{content}")
        print(f"  â””â”€ âœ… ì €ì¥ ì™„ë£Œ: {file_name} ({title})")
    except Exception as e:
        print(f"  â””â”€ âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    time.sleep(DELAY)

# ì‹¤í–‰
print("ğŸš€ ë ˆë‹Œ ì „ì§‘ ì „ì²´ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ìƒë‹¹í•œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")
crawl(START_URL, 0)
print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ! ì´ {len(visited_urls)}ê°œì˜ í˜ì´ì§€ë¥¼ ê²€ì‚¬í–ˆìŠµë‹ˆë‹¤.")