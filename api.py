import asyncio
import json
import os

import uvicorn
from fastapi import FastAPI, Query, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
#from sse_starlette.sse import EventSourceResponse
from langchain_core.messages import HumanMessage

app = FastAPI(title="Cyber-Lenin API")

# Lazy-load chatbot so uvicorn can bind the port first
_graph = None
_supabase = None


def get_graph():
    global _graph
    if _graph is None:
        from chatbot import graph
        _graph = graph
    return _graph


def get_supabase():
    global _supabase
    if _supabase is None:
        from chatbot import supabase
        _supabase = supabase
    return _supabase


@app.api_route("/", methods=["GET", "HEAD"])
async def health():
    return {"status": "ok"}


@app.api_route("/api/health", methods=["GET", "HEAD"])
async def api_health():
    return {"status": "ok"}


app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://bichonwebpage.onrender.com",
    "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ChatRequest(BaseModel):
    message: str

def format_sse(data: dict):
    """Server-Sent Events í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"


@app.post("/chat")
async def chat(request: ChatRequest):
    """
    í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ ë¡œê·¸ì™€ ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    """
    _graph = get_graph()

    async def event_generator():
        inputs = {"messages": [HumanMessage(content=request.message)]}

        # ê·¸ë˜í”„ ì‹¤í–‰ ë° ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° (stream_mode="updates")
        # ê° ë…¸ë“œê°€ ëë‚  ë•Œë§ˆë‹¤ ê·¸ ë…¸ë“œì˜ ì¶œë ¥ê°’(logs ë“±)ì„ ë°›ì•„ì˜µë‹ˆë‹¤.
        async for output in _graph.astream(inputs, stream_mode="updates"):
            for node_name, node_content in output.items():
                # log_conversation ë…¸ë“œëŠ” ë‚´ë¶€ ì „ìš©ì´ë¯€ë¡œ í´ë¼ì´ì–¸íŠ¸ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ
                if node_name == "log_conversation":
                    continue

                # ë¡œê·¸ê°€ ìˆë‹¤ë©´ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
                if "logs" in node_content:
                    for log_line in node_content["logs"]:
                        yield format_sse({
                            "type": "log",
                            "node": node_name,
                            "content": log_line
                        })
                
                # ìµœì¢… ë‹µë³€ ìƒì„± ë‹¨ê³„ë¼ë©´ ë‹µë³€ ë‚´ìš© ì „ì†¡
                if node_name == "generate":
                    last_message = node_content["messages"][-1]
                    yield format_sse({
                        "type": "answer",
                        "content": last_message.content
                    })

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@app.get("/logs")
async def get_logs(
    limit: int = Query(default=50, ge=1, le=500),
    offset: int = Query(default=0, ge=0),
):
    """
    Fetch chat logs from Supabase, ordered by most recent first.
    """
    sb = get_supabase()
    result = (
        sb.table("chat_logs")
        .select("*")
        .order("created_at", desc=True)
        .range(offset, offset + limit - 1)
        .execute()
    )
    return {"logs": result.data, "count": len(result.data)}


if __name__ == "__main__":
    print("ğŸš© ì‚¬ì´ë²„-ë ˆë‹Œ API ì„œë²„ ê°€ë™... (Port: 8000)")
    uvicorn.run(app, host="0.0.0.0", port=8000)