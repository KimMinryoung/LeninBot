"""
í•œêµ­ êµ­ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘ + ì¸ë¬¼ í”„ë¡œíŒŒì¼ ë³´ê°• â†’ ì§€ì‹ê·¸ë˜í”„ ìˆ˜ì§‘
==========================================================

Functions:
  fetch_kr_news()                  â€” Tavilyë¡œ í•œêµ­ êµ­ë‚´ ë‰´ìŠ¤ ê²€ìƒ‰
  extract_persons_from_articles()  â€” LLMìœ¼ë¡œ ê¸°ì‚¬ì—ì„œ ì¸ë¬¼ëª… ì¶”ì¶œ
  fetch_person_profile()           â€” Tavilyë¡œ ì¸ë¬¼ í”„ë¡œíŒŒì¼ ê²€ìƒ‰
  ingest_news_to_kg()              â€” ë‰´ìŠ¤ ê¸°ì‚¬ KG ìˆ˜ì§‘
  ingest_profile_to_kg()           â€” í”„ë¡œíŒŒì¼ KG ìˆ˜ì§‘
  run_full_pipeline()              â€” ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""

import asyncio
import json
import os
import re
from datetime import datetime, timezone
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()

from langchain_tavily import TavilySearch
from graphiti_core.llm_client.gemini_client import GeminiClient
from graphiti_core.llm_client.config import LLMConfig, ModelSize
from graphiti_core.prompts.models import Message

from .service import GraphMemoryService


# ============================================================
# LLM í´ë¼ì´ì–¸íŠ¸ (ì¸ë¬¼ ì¶”ì¶œìš©)
# ============================================================

def _get_llm_client() -> GeminiClient:
    """ì¸ë¬¼ ì¶”ì¶œ ì „ìš© ê²½ëŸ‰ LLM í´ë¼ì´ì–¸íŠ¸."""
    return GeminiClient(
        config=LLMConfig(
            api_key=os.getenv("GEMINI_API_KEY", ""),
            model="gemini-2.5-flash",
            small_model="gemini-2.5-flash-lite",
        )
    )


def _extract_text_from_response(response) -> str:
    """LLM ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (service.py íŒ¨í„´ ì¬ì‚¬ìš©)."""
    if response is None:
        return ""
    if isinstance(response, str):
        return response.strip()
    for attr in ("text", "content", "output_text", "response"):
        value = getattr(response, attr, None)
        if isinstance(value, str) and value.strip():
            return value.strip()
    if isinstance(response, dict):
        for key in ("text", "content", "output_text", "response"):
            value = response.get(key)
            if isinstance(value, str) and value.strip():
                return value.strip()
    return str(response).strip()


def _parse_json_from_text(text: str):
    """í…ìŠ¤íŠ¸ì—ì„œ JSON íŒŒì‹± (ì½”ë“œíœìŠ¤ ì²˜ë¦¬)."""
    raw = text.strip()
    if not raw:
        return None
    if raw.startswith("```"):
        raw = raw.strip("`")
        if raw.lower().startswith("json"):
            raw = raw[4:]
        raw = raw.strip()
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        pass
    # {} ë˜ëŠ” [] ì°¾ê¸°
    for open_ch, close_ch in [("{", "}"), ("[", "]")]:
        start = raw.find(open_ch)
        end = raw.rfind(close_ch)
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(raw[start:end + 1])
            except json.JSONDecodeError:
                continue
    return None


# ============================================================
# 1. í•œêµ­ êµ­ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘
# ============================================================

_KOREA_KEYWORDS = {
    "korea", "korean", "seoul", "busan", "lee jae", "ì´ì¬ëª…",
    "yoon", "ìœ¤ì„ì—´", "rok", "south korea", "pyongyang", "dprk",
}


def _is_korea_relevant(title: str, content: str) -> bool:
    """title + content ì• 500ìì—ì„œ í•œêµ­ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ í™•ì¸í•œë‹¤."""
    text = (title + " " + content[:500]).lower()
    return any(kw in text for kw in _KOREA_KEYWORDS)


async def _extract_article_body(raw_content: str) -> str:
    """Tavily raw_contentì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ë§Œ LLMìœ¼ë¡œ ì¶”ì¶œí•œë‹¤.

    ì‚¬ì´ë“œë°”, ê´‘ê³ , ê´€ë ¨ ê¸°ì‚¬, í‘¸í„° ë“± ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³ 
    ê¸°ì‚¬ ì œëª© + ë³¸ë¬¸ë§Œ ë°˜í™˜í•œë‹¤.
    """
    if not raw_content or len(raw_content) < 100:
        return raw_content or ""

    llm = _get_llm_client()
    prompt = (
        "Extract ONLY the main article headline and body text from the web page content below.\n"
        "Remove ALL of the following: navigation menus, sidebars, advertisements, "
        "related articles, footer, author bios, social media links, comments, "
        "cookie notices, subscription prompts.\n\n"
        "Return ONLY the cleaned article text (headline + body). "
        "Do NOT add any commentary or formatting.\n\n"
        f"[WEB PAGE CONTENT]\n{raw_content[:6000]}"
    )

    try:
        response = await llm.generate_response(
            [Message(role="user", content=prompt)],
            model_size=ModelSize.small,
        )
        extracted = _extract_text_from_response(response)
        if extracted and len(extracted) > 50:
            return extracted
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì •ì œ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}", flush=True)

    return raw_content


def _sanitize_filename(text: str, max_len: int = 60) -> str:
    """íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•œë‹¤."""
    sanitized = re.sub(r'[<>:"/\\|?*\n\r\t]', '', text)
    sanitized = re.sub(r'\s+', '_', sanitized.strip())
    return sanitized[:max_len]


async def fetch_kr_news(
    query: str = "South Korea president politics economy 2026",
    max_results: int = 5,
    time_range: str = "week",
    save_dir: str | None = "docs/news",
) -> list[dict]:
    """Tavilyë¡œ í•œêµ­ êµ­ë‚´ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ë³¸ë¬¸ì„ ì •ì œí•œë‹¤.

    Args:
        save_dir: ì •ì œëœ ê¸°ì‚¬ë¥¼ ì €ì¥í•  ë””ë ‰í„°ë¦¬. Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ.

    Returns:
        [{"title": str, "url": str, "content": str}, ...]
    """
    tavily = TavilySearch(
        max_results=max_results,
        topic="news",
        search_depth="advanced",
        include_raw_content=True,
        time_range=time_range,
    )

    print(f"ğŸ” í•œêµ­ ë‰´ìŠ¤ ê²€ìƒ‰: '{query}' (max={max_results})", flush=True)
    try:
        raw = await tavily.ainvoke(query)
    except Exception as e:
        print(f"âš ï¸ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}", flush=True)
        return []

    if isinstance(raw, str):
        print(f"âš ï¸ Tavily ë¬¸ìì—´ ë°˜í™˜: {raw[:200]}", flush=True)
        return []

    results = raw.get("results", []) if isinstance(raw, dict) else []
    articles = []
    for r in results:
        content = r.get("raw_content") or r.get("content", "")
        if not content or len(content) < 50:
            continue
        articles.append({
            "title": r.get("title", ""),
            "url": r.get("url", ""),
            "content": content,
        })

    # ê´€ë ¨ì„± í•„í„°: í•œêµ­ ê´€ë ¨ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê¸°ì‚¬ ì œê±°
    before = len(articles)
    articles = [a for a in articles if _is_korea_relevant(a["title"], a["content"])]
    if before != len(articles):
        print(f"ğŸ” ê´€ë ¨ì„± í•„í„°: {before}ê±´ â†’ {len(articles)}ê±´", flush=True)

    # ë³¸ë¬¸ ì •ì œ: LLMìœ¼ë¡œ ì‚¬ì´ë“œë°”/ê´‘ê³ /ê´€ë ¨ê¸°ì‚¬ ë…¸ì´ì¦ˆ ì œê±°
    print(f"ğŸ§¹ {len(articles)}ê±´ ë³¸ë¬¸ ì •ì œ ì¤‘...", flush=True)
    for art in articles:
        art["content"] = await _extract_article_body(art["content"])

    # docs/news/ì— ì •ì œëœ ê¸°ì‚¬ ì €ì¥
    if save_dir and articles:
        save_path = Path(save_dir)
        if not save_path.is_absolute():
            project_root = Path(__file__).resolve().parent.parent
            save_path = project_root / save_dir
        save_path.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        for i, art in enumerate(articles):
            fname = f"{timestamp}_{i}_{_sanitize_filename(art['title'])}.txt"
            fpath = save_path / fname
            file_content = f"{art['url']}\n{art['title']}\n---\n{art['content']}"
            fpath.write_text(file_content, encoding="utf-8")
        print(f"ğŸ’¾ {len(articles)}ê±´ ì €ì¥ â†’ {save_path}", flush=True)

    print(f"ğŸ“° {len(articles)}ê±´ ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì‹ ", flush=True)
    return articles


# ============================================================
# 2. ê¸°ì‚¬ì—ì„œ ì¸ë¬¼ëª… ì¶”ì¶œ
# ============================================================

PERSON_EXTRACTION_PROMPT = """\
Extract **key South Korean public figures** mentioned in the news articles below.

[RULES]
1. Only South Korean nationals or people active in South Korea
2. Prioritize: politicians, business leaders, senior officials, party leaders
3. Deduplicate (each person appears once)
4. Maximum 5 persons
5. If no South Korean person is explicitly named, look for implied references to South Korean leaders, presidents, party heads, etc.
6. You MUST return at least 1 person if the article mentions South Korea at all â€” infer the most relevant leader (e.g., the current South Korean president or opposition leader)

[OUTPUT FORMAT â€” JSON array ONLY, no other text]
[
  {{"name_ko": "ì´ì¬ëª…", "name_en": "Lee Jae-myung", "role": "Leader of the Democratic Party of Korea"}},
  ...
]

[ARTICLES]
{articles_text}
"""


async def extract_persons_from_articles(articles: list[dict]) -> list[dict]:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ í•œêµ­ ì¸ë¬¼ëª…ì„ LLMìœ¼ë¡œ ì¶”ì¶œí•œë‹¤.

    Returns:
        [{"name_ko": str, "name_en": str, "role": str}, ...]
    """
    # ê¸°ì‚¬ ë³¸ë¬¸ í•©ì¹˜ê¸° (ê° ìµœëŒ€ 800ì)
    parts = []
    for i, art in enumerate(articles):
        snippet = art["content"][:800]
        parts.append(f"--- ê¸°ì‚¬ {i+1}: {art['title']} ---\n{snippet}")
    articles_text = "\n\n".join(parts)

    llm = _get_llm_client()
    prompt = PERSON_EXTRACTION_PROMPT.format(articles_text=articles_text)

    print("ğŸ§  LLM ì¸ë¬¼ ì¶”ì¶œ ì¤‘...", flush=True)
    response = await llm.generate_response([Message(role="user", content=prompt)])
    text = _extract_text_from_response(response)
    parsed = _parse_json_from_text(text)

    if not isinstance(parsed, list):
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, raw: {text[:300]}", flush=True)
        return []

    persons = []
    for item in parsed:
        if isinstance(item, dict) and "name_ko" in item:
            persons.append({
                "name_ko": item.get("name_ko", ""),
                "name_en": item.get("name_en", ""),
                "role": item.get("role", "unknown"),
            })

    print(f"ğŸ‘¤ {len(persons)}ëª… ì¶”ì¶œ ì™„ë£Œ", flush=True)
    return persons


# ============================================================
# 3. ì¸ë¬¼ í”„ë¡œíŒŒì¼ Tavily ê²€ìƒ‰
# ============================================================

PROFILE_SYNTHESIS_PROMPT = """\
Synthesize the search results below into a person profile.

CRITICAL: The search results below contain the MOST RECENT information.
If the search results state a person holds a specific position (e.g., president),
ALWAYS use that information, even if it differs from your training data.
Your training data may be outdated â€” the search results are authoritative.

[RULES]
1. Write in English (for knowledge graph ingestion)
2. ONLY use facts from the search results below (no speculation, no training data)
3. Structure: name, CURRENT role/position, party/organization, career highlights, recent activities
4. For the "role" field, use the person's CURRENT position as stated in the search results
5. For the "summary" field, START with the person's CURRENT position as stated in search results

[OUTPUT FORMAT â€” JSON only, no other text]
{{
  "name": "Lee Jae-myung",
  "name_ko": "ì´ì¬ëª…",
  "role": "President of South Korea",
  "party_or_org": "Democratic Party of Korea",
  "nationality": "KR",
  "summary": "Lee Jae-myung is the President of South Korea ... (200-400 words, fact-based, English)"
}}

[SEARCH RESULTS]
{search_results}
"""


async def fetch_person_profile(
    name_ko: str,
    name_en: str,
) -> dict:
    """Tavily ê²€ìƒ‰ìœ¼ë¡œ ì¸ë¬¼ í”„ë¡œíŒŒì¼ì„ ìˆ˜ì§‘ â†’ LLMìœ¼ë¡œ êµ¬ì¡°í™”í•œë‹¤.

    Returns:
        {"name": str, "name_ko": str, "role": str,
         "party_or_org": str, "nationality": str, "summary": str}
    """
    # Tavily ì¼ë°˜ ê²€ìƒ‰ (ë‰´ìŠ¤ê°€ ì•„ë‹Œ general)
    tavily = TavilySearch(
        max_results=5,
        topic="general",
        search_depth="advanced",
        include_raw_content=True,
    )

    query = f"{name_en} {name_ko} current position role president 2026"
    print(f"ğŸ” í”„ë¡œíŒŒì¼ ê²€ìƒ‰: '{query}'", flush=True)

    try:
        raw = await tavily.ainvoke(query)
    except Exception as e:
        print(f"âš ï¸ Tavily í”„ë¡œíŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", flush=True)
        return {"name": name_en, "name_ko": name_ko, "summary": f"Profile search failed: {e}"}

    if isinstance(raw, str):
        results_text = raw[:3000]
    else:
        items = raw.get("results", []) if isinstance(raw, dict) else []
        parts = []
        for r in items:
            content = r.get("raw_content") or r.get("content", "")
            parts.append(f"[{r.get('title', '')}]\n{content[:600]}")
        results_text = "\n\n".join(parts)

    if not results_text or len(results_text) < 30:
        return {
            "name": name_en, "name_ko": name_ko,
            "role": "unknown", "party_or_org": "unknown",
            "nationality": "KR",
            "summary": f"Insufficient search results for {name_en}.",
        }

    # LLMìœ¼ë¡œ í”„ë¡œíŒŒì¼ ì¢…í•©
    llm = _get_llm_client()
    prompt = PROFILE_SYNTHESIS_PROMPT.format(search_results=results_text[:4000])

    print("ğŸ§  LLM í”„ë¡œíŒŒì¼ ì¢…í•© ì¤‘...", flush=True)
    response = await llm.generate_response([Message(role="user", content=prompt)])
    text = _extract_text_from_response(response)
    parsed = _parse_json_from_text(text)

    if isinstance(parsed, dict) and "summary" in parsed:
        parsed.setdefault("name", name_en)
        parsed.setdefault("name_ko", name_ko)
        parsed.setdefault("nationality", "KR")
        print(f"âœ… í”„ë¡œíŒŒì¼ ì¢…í•© ì™„ë£Œ: {parsed['name']}", flush=True)
        return parsed

    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ raw textë¥¼ summaryë¡œ
    return {
        "name": name_en, "name_ko": name_ko,
        "role": "unknown", "party_or_org": "unknown",
        "nationality": "KR",
        "summary": text[:500] if text else "Profile synthesis failed.",
    }


# ============================================================
# 4. ë‰´ìŠ¤ ê¸°ì‚¬ KG ìˆ˜ì§‘
# ============================================================

async def ingest_news_to_kg(
    service: GraphMemoryService,
    articles: list[dict],
    group_id: str = "korea_domestic",
    delay_between: float = 5.0,
) -> dict:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì„ KGì— ì—í”¼ì†Œë“œë¡œ ìˆ˜ì§‘í•œë‹¤.

    Returns:
        {"succeeded": int, "failed": int, "articles": [{"title", "status"}]}
    """
    ref_time = datetime.now(timezone.utc)
    succeeded = 0
    failed = 0
    article_log = []

    for i, art in enumerate(articles):
        title = art["title"]
        body = f"{title}\n\n{art['content']}"
        name = f"kr-news-{i}"

        print(f"\n[{i+1}/{len(articles)}] ë‰´ìŠ¤ ìˆ˜ì§‘: {title[:60]}", flush=True)

        try:
            await service.ingest_episode(
                name=name,
                body=body,
                source_type="osint_news",
                reference_time=ref_time,
                group_id=group_id,
                source_description=f"Korean news: {art.get('url', '')}",
                preprocess_news=True,
                max_body_chars=1500,
            )
            succeeded += 1
            article_log.append({"title": title[:120], "status": "ok"})
            print(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ", flush=True)
        except Exception as e:
            import traceback
            failed += 1
            err_msg = f"{type(e).__name__}: {e}"
            article_log.append({"title": title[:120], "status": f"failed: {err_msg}"})
            print(f"  âŒ ì‹¤íŒ¨: {err_msg[:300]}", flush=True)
            traceback.print_exc()

        # ë‹¤ìŒ ì—í”¼ì†Œë“œ ì „ ëŒ€ê¸° (rate limit)
        if i < len(articles) - 1:
            print(f"  â³ {delay_between}ì´ˆ ëŒ€ê¸°...", flush=True)
            await asyncio.sleep(delay_between)

    print(f"\nğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼: {succeeded} ì„±ê³µ, {failed} ì‹¤íŒ¨", flush=True)
    return {"succeeded": succeeded, "failed": failed, "articles": article_log}


# ============================================================
# 5. í”„ë¡œíŒŒì¼ KG ìˆ˜ì§‘
# ============================================================

async def ingest_profile_to_kg(
    service: GraphMemoryService,
    profile: dict,
    group_id: str = "korea_domestic",
) -> dict:
    """ì¸ë¬¼ í”„ë¡œíŒŒì¼ì„ KGì— ì—í”¼ì†Œë“œë¡œ ìˆ˜ì§‘í•œë‹¤.

    Returns:
        {"status": "ok"|"failed", "name": str}
    """
    name = profile.get("name", "unknown")
    summary = profile.get("summary", "")
    role = profile.get("role", "")
    party = profile.get("party_or_org", "")

    body = (
        f"Person Profile: {name}\n"
        f"Korean name: {profile.get('name_ko', '')}\n"
        f"Role: {role}\n"
        f"Organization: {party}\n"
        f"Nationality: {profile.get('nationality', 'KR')}\n\n"
        f"{summary}"
    )

    print(f"ğŸ“ í”„ë¡œíŒŒì¼ KG ìˆ˜ì§‘: {name}", flush=True)
    ref_time = datetime.now(timezone.utc)

    try:
        await service.ingest_episode(
            name=f"profile-{name.lower().replace(' ', '-')}",
            body=body,
            source_type="osint_news",
            reference_time=ref_time,
            group_id=group_id,
            source_description=f"Person profile: {name}",
            preprocess_news=False,   # í”„ë¡œíŒŒì¼ì€ ì´ë¯¸ ì •ì œë¨
            max_body_chars=2000,
        )
        print(f"  âœ… í”„ë¡œíŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ: {name}", flush=True)
        return {"status": "ok", "name": name}
    except Exception as e:
        print(f"  âŒ í”„ë¡œíŒŒì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:200]}", flush=True)
        return {"status": "failed", "name": name, "error": str(e)}


# ============================================================
# ì „ì²´ íŒŒì´í”„ë¼ì¸ (standalone ì‹¤í–‰ìš©)
# ============================================================

async def run_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸: ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ì¸ë¬¼ ì¶”ì¶œ â†’ í”„ë¡œíŒŒì¼ ë³´ê°• â†’ KG ìˆ˜ì§‘."""
    service = GraphMemoryService()
    await service.initialize()

    try:
        # 1. í•œêµ­ ë‰´ìŠ¤ 3ê±´ ìˆ˜ì§‘
        print("\n" + "=" * 60)
        print("Phase 1: í•œêµ­ êµ­ë‚´ ë‰´ìŠ¤ ìˆ˜ì§‘")
        print("=" * 60)
        articles = await fetch_kr_news(
            max_results=5,
            time_range="week",
        )
        if not articles:
            print("âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨. ì¤‘ë‹¨.")
            return

        # 2. ì¸ë¬¼ ì¶”ì¶œ
        print("\n" + "=" * 60)
        print("Phase 2: ì¸ë¬¼ëª… ì¶”ì¶œ")
        print("=" * 60)
        persons = await extract_persons_from_articles(articles)
        print(f"ì¶”ì¶œëœ ì¸ë¬¼: {[p['name_ko'] for p in persons]}")

        # 3. ë‰´ìŠ¤ KG ìˆ˜ì§‘ (ìµœëŒ€ 3ê±´)
        print("\n" + "=" * 60)
        print("Phase 3: ë‰´ìŠ¤ KG ìˆ˜ì§‘")
        print("=" * 60)
        news_result = await ingest_news_to_kg(
            service=service,
            articles=articles[:3],
            group_id="korea_domestic",
        )

        # 4. ì¸ë¬¼ í”„ë¡œíŒŒì¼ ë³´ê°• + KG ìˆ˜ì§‘
        print("\n" + "=" * 60)
        print("Phase 4: ì¸ë¬¼ í”„ë¡œíŒŒì¼ ë³´ê°• â†’ KG ìˆ˜ì§‘")
        print("=" * 60)
        for p in persons[:2]:  # ìƒìœ„ 2ëª…
            print(f"\n--- í”„ë¡œíŒŒì¼ ìˆ˜ì§‘: {p['name_ko']} ---")
            await asyncio.sleep(5)  # rate limit ëŒ€ê¸°
            profile = await fetch_person_profile(p["name_ko"], p["name_en"])
            await asyncio.sleep(5)
            await ingest_profile_to_kg(service, profile, group_id="korea_domestic")

        # 5. ê²€ì¦
        print("\n" + "=" * 60)
        print("Phase 5: KG ê²€ì¦")
        print("=" * 60)
        search_result = await service.search(
            query="South Korea politics economy leader president party",
            group_ids=["korea_domestic"],
            num_results=15,
        )
        print(f"ë…¸ë“œ {len(search_result['nodes'])}ê°œ, ì—£ì§€ {len(search_result['edges'])}ê°œ")
        for n in search_result["nodes"][:10]:
            print(f"  [node] {n['name']} ({', '.join(n['labels'])})")
        for e in search_result["edges"][:10]:
            print(f"  [edge] {e['fact'][:80]}")

        print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

    finally:
        await service.close()


if __name__ == "__main__":
    asyncio.run(run_full_pipeline())
