import os
import glob
import re
from dotenv import load_dotenv
from tqdm import tqdm
from supabase.client import Client, create_client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import torch

load_dotenv()

# 1. ì´ˆê¸°í™”
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# BGE-M3 ì„ë² ë”© ëª¨ë¸ (1024ì°¨ì›, ë‹¤êµ­ì–´ ì§€ì›)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[System] Using device: {device}")

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True}
)
vectorstore = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="lenin_corpus",
    query_name="match_documents",
)

source_directory = "./docs/lenin"
log_file = "processed_files.txt"

# 2. íŒŒì¼ í™•ì¥ìë³„ ë¡œë” ë§¤í•‘ (í´ë˜ìŠ¤ ìì²´ë¥¼ ë§¤í•‘)
LOADER_MAPPING = {
    ".pdf": PyPDFLoader,
    ".txt": TextLoader
}

# ì €ì-ë””ë ‰í† ë¦¬ ë§¤í•‘ (source_directory ê²½ë¡œë¡œ íŒë³„)
AUTHOR_BY_DIR = {
    "lenin": "Lenin",
    "marx_engels": "Marx & Engels",
}

# ì´ë¡ ê°€ prefix â†’ ì €ì ì´ë¦„ ë§¤í•‘
THEORIST_AUTHORS = {
    "trotsky": "Trotsky",
    "luxemburg": "Rosa Luxemburg",
    "gramsci": "Gramsci",
    "bukharin": "Bukharin",
    "mao": "Mao",
}

# modern_analysis prefix â†’ ì €ì/ì¶œì²˜ ë§¤í•‘
MODERN_AUTHORS = {
    "arxiv": None,       # íŒŒì¼ ë‚´ Authors: í—¤ë”ì—ì„œ ì¶”ì¶œ
    "bis": "BIS",
    "mxo_mandel": "Ernest Mandel",   # ë” êµ¬ì²´ì ì¸ prefixë¥¼ ë¨¼ì € ë°°ì¹˜
    "mxo_marcuse": "Herbert Marcuse",
    "mxo": "Marxists.org",
    "uprising": "Uprising(ë°˜ë€) - Korean Marx-Lenin-Maoism group",
    "bolky": "Bolky Group (ë³¼ì…°ë¹„í‚¤ê·¸ë£¹) - Korean Trotskyist organization",
}


def _extract_author_year(file_path, source_dir):
    """íŒŒì¼ ê²½ë¡œì™€ í—¤ë”ì—ì„œ author, year ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•œë‹¤."""
    file_name = os.path.basename(file_path)
    base_name = os.path.splitext(file_name)[0]
    author = None
    year = None

    # --- ë””ë ‰í† ë¦¬ ê¸°ë°˜ ì €ì íŒë³„ ---
    dir_name = os.path.basename(os.path.normpath(source_dir))
    if dir_name in AUTHOR_BY_DIR:
        author = AUTHOR_BY_DIR[dir_name]

    # --- ì´ë¡ ê°€ prefix ê¸°ë°˜ ì €ì íŒë³„ ---
    if dir_name == "theorists":
        for prefix, name in THEORIST_AUTHORS.items():
            if base_name.startswith(prefix + "_"):
                author = name
                break

    # --- modern_analysis prefix ê¸°ë°˜ ì €ì íŒë³„ ---
    if dir_name == "modern_analysis":
        for prefix, name in MODERN_AUTHORS.items():
            if base_name.startswith(prefix + "_"):
                author = name  # arxivì€ None â†’ ì•„ë˜ í—¤ë” íŒŒì‹±ì—ì„œ ë®ì–´ì”€
                break

    # --- íŒŒì¼ í—¤ë” íŒŒì‹± (Source/Title/Author(s)/Year ì¤„) ---
    source_url = ""
    title_line = ""
    year_from_header = None
    try:
        with open(file_path, 'r', encoding='utf-8') as tf:
            for line in tf:
                line_s = line.strip()
                if line_s.startswith("Source:"):
                    source_url = line_s[7:].strip()
                elif line_s.startswith("Title:"):
                    title_line = line_s[6:].strip()
                elif line_s.startswith("Authors:") and not author:
                    author = line_s[8:].strip()
                elif line_s.startswith("Author:") and not author:
                    author = line_s[7:].strip()
                elif line_s.startswith("Year:") and not year_from_header:
                    year_from_header = line_s[5:].strip()
                elif not line_s.startswith(("Source:", "Title:", "Authors:", "Author:", "Year:", "")):
                    break  # í—¤ë” ì˜ì—­ì„ ë²—ì–´ë‚˜ë©´ ì¤‘ë‹¨
    except Exception:
        pass

    # --- ì—°ë„ ì¶”ì¶œ: í—¤ë”ì˜ Year: í•„ë“œ (ê°€ì¥ ìš°ì„ ) ---
    if year_from_header:
        year = year_from_header

    # --- ì—°ë„ ì¶”ì¶œ: íŒŒì¼ëª… ì• 4ìë¦¬ ---
    if not year:
        year_match = re.match(r'^(\d{4})_', base_name)
        if year_match:
            year = year_match.group(1)

    # --- ì—°ë„ ì¶”ì¶œ fallback: Source URLì—ì„œ /YYYY/ íŒ¨í„´ ---
    if not year and source_url:
        url_year = re.search(r'/(\d{4})/', source_url)
        if url_year:
            year = url_year.group(1)

    # --- ì—°ë„ ì¶”ì¶œ fallback: arXiv URLì—ì„œ YYMM íŒ¨í„´ (e.g. arxiv.org/abs/2304.07859) ---
    if not year and source_url and 'arxiv.org/abs/' in source_url:
        arxiv_match = re.search(r'arxiv\.org/abs/(\d{2})\d{2}\.', source_url)
        if arxiv_match:
            year = "20" + arxiv_match.group(1)

    # --- ì—°ë„ ì¶”ì¶œ fallback: Titleì—ì„œ ì—°ë„ íŒ¨í„´ ---
    if not year and title_line:
        title_year = re.search(r'\b(1[7-9]\d{2}|20[0-2]\d)\b', title_line)
        if title_year:
            year = title_year.group(1)

    return author, year


def update_knowledge(layer="core_theory"):
    print(f"ğŸ“‚ {source_directory} í´ë”ì—ì„œ ìƒˆ ë¬¸ì„œë¥¼ íƒìƒ‰ ì¤‘... (layer: {layer})")
    
    # ë¡œê·¸ íŒŒì¼ì—ì„œ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì½ê¸°
    if os.path.exists(log_file):
        with open(log_file, "r", encoding="utf-8") as f:
            processed_files = set(f.read().splitlines())
    else:
        processed_files = set()

    # í•˜ìœ„ í´ë”ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë“  .txt, .pdf íŒŒì¼ ì°¾ê¸°
    all_files = []
    for ext in LOADER_MAPPING.keys():
        all_files.extend(glob.glob(os.path.join(source_directory, f"**/*{ext}"), recursive=True))

    # ê²½ë¡œë¥¼ í†µì¼í•˜ì—¬ ë¹„êµ (ì—­ìŠ¬ë˜ì‹œ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ os.path.normpath ì‚¬ìš©)
    new_files = [f for f in all_files if os.path.normpath(f) not in processed_files]

    if not new_files:
        print("âœ… ì¶”ê°€í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_chunks = 0
    file_bar = tqdm(new_files, desc="íŒŒì¼ ì²˜ë¦¬", unit="file", position=0)
    for file_path in file_bar:
        file_name = os.path.basename(file_path)
        file_bar.set_postfix_str(file_name[:40])
        ext = os.path.splitext(file_name)[1].lower()

        try:
            # 3. ë‹¨ì¼ íŒŒì¼ ë¡œë” ì‹¤í–‰
            if ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                loader = LOADER_MAPPING[ext](file_path)

            docs = loader.load()

            # 4. í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            splits = text_splitter.split_documents(docs)

            # 4.5. ë©”íƒ€ë°ì´í„°ì— layer, source, author, year ì£¼ì…
            # íŒŒì¼ ë‚´ Title: í—¤ë”ê°€ ìˆìœ¼ë©´ ë¬¸í—Œ ì œëª©ì„ sourceë¡œ ì‚¬ìš©
            source_name = os.path.splitext(file_name)[0]
            try:
                with open(file_path, 'r', encoding='utf-8') as tf:
                    for line in tf:
                        line = line.strip()
                        if line.startswith("Title:") and line[6:].strip():
                            source_name = line[6:].strip()
                            break
                        if not line.startswith("Source:") and line:
                            break  # í—¤ë” ì˜ì—­ì„ ë²—ì–´ë‚˜ë©´ ì¤‘ë‹¨
            except Exception:
                pass

            author, year = _extract_author_year(file_path, source_directory)

            for doc in splits:
                doc.metadata["layer"] = layer
                doc.metadata["source"] = source_name
                if author:
                    doc.metadata["author"] = author
                if year:
                    doc.metadata["year"] = year

            # 5. Supabase ì „ì†¡ (ë°°ì¹˜ ì²˜ë¦¬)
            batch_size = 5
            for i in range(0, len(splits), batch_size):
                vectorstore.add_documents(documents=splits[i:i+batch_size])
            total_chunks += len(splits)

            # 6. ë¡œê·¸ì— ì¶”ê°€ (ì„±ê³µ ì‹œì—ë§Œ)
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(os.path.normpath(file_path) + "\n")

        except Exception as e:
            tqdm.write(f"âŒ ì—ëŸ¬ ë°œìƒ ({file_name}): {e}")
    file_bar.close()

    print(f"\nâœ¨ ì§€ì‹ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì´ {len(new_files)}ê°œ ë¬¸ì„œ, {total_chunks}ê°œ ì²­í¬ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--layer", default="core_theory",
                        help="Metadata layer tag (e.g. core_theory, modern_analysis)")
    parser.add_argument("--source-dir",
                        help="Override source directory (default: ./docs/lenin)")
    args = parser.parse_args()
    if args.source_dir:
        source_directory = args.source_dir  # noqa: F841 - used by update_knowledge
    update_knowledge(layer=args.layer)