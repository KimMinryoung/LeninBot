import os
import glob
from dotenv import load_dotenv
from tqdm import tqdm
from supabase.client import Client, create_client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# 1. ì´ˆê¸°í™”
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
vectorstore = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="lenin_corpus",
    query_name="match_documents",
)

source_directory = "./docs/lenin"
log_file = "processed_files.txt"

# 2. íŒŒì¼ í™•ì¥ìë³„ ë¡œë” ë§¤í•‘ (í´ë˜ìŠ¤ ìì²´ë¥¼ ë§¤í•‘)
LOADER_MAPPING = {
    ".pdf": PyPDFLoader,
    ".txt": TextLoader
}

def update_knowledge(layer="core_theory"):
    print(f"ğŸ“‚ {source_directory} í´ë”ì—ì„œ ìƒˆ ë¬¸ì„œë¥¼ íƒìƒ‰ ì¤‘... (layer: {layer})")
    
    # ë¡œê·¸ íŒŒì¼ì—ì„œ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì½ê¸°
    if os.path.exists(log_file):
        with open(log_file, "r", encoding="utf-8") as f:
            processed_files = set(f.read().splitlines())
    else:
        processed_files = set()

    # í•˜ìœ„ í´ë”ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë“  .txt, .pdf íŒŒì¼ ì°¾ê¸°
    all_files = []
    for ext in LOADER_MAPPING.keys():
        all_files.extend(glob.glob(os.path.join(source_directory, f"**/*{ext}"), recursive=True))

    # ê²½ë¡œë¥¼ í†µì¼í•˜ì—¬ ë¹„êµ (ì—­ìŠ¬ë˜ì‹œ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ os.path.normpath ì‚¬ìš©)
    new_files = [f for f in all_files if os.path.normpath(f) not in processed_files]

    if not new_files:
        print("âœ… ì¶”ê°€í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file_path in tqdm(new_files, desc="ì „ì²´ ì§„í–‰ë¥ "):
        file_name = os.path.basename(file_path)
        ext = os.path.splitext(file_name)[1].lower()
        
        try:
            # 3. ë‹¨ì¼ íŒŒì¼ ë¡œë” ì‹¤í–‰
            if ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                loader = LOADER_MAPPING[ext](file_path)
            
            docs = loader.load()
        
            # 4. í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            splits = text_splitter.split_documents(docs)

            # 4.5. ë©”íƒ€ë°ì´í„°ì— layer ë° source ì£¼ì…
            source_name = os.path.splitext(file_name)[0]
            for doc in splits:
                doc.metadata["layer"] = layer
                doc.metadata["source"] = source_name

            # 5. Supabase ì „ì†¡ (ë°°ì¹˜ ì²˜ë¦¬)
            # tqdmì„ ì¤‘ì²©í•´ì„œ ì“°ì§€ ì•Šê³  íŒŒì¼ ë‹¨ìœ„ë¡œë§Œ í‘œì‹œí•˜ê±°ë‚˜, ë‚´ë¶€ ì „ì†¡ë„ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            for i in range(0, len(splits), 100):
                vectorstore.add_documents(documents=splits[i:i+100])
            
            # 6. ë¡œê·¸ì— ì¶”ê°€ (ì„±ê³µ ì‹œì—ë§Œ)
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(os.path.normpath(file_path) + "\n")
                
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ ({file_name}): {e}")

    print(f"\nâœ¨ ì§€ì‹ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì´ {len(new_files)}ê°œì˜ ë¬¸ì„œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--layer", default="core_theory",
                        help="Metadata layer tag (e.g. core_theory, modern_analysis)")
    parser.add_argument("--source-dir",
                        help="Override source directory (default: ./docs/lenin)")
    args = parser.parse_args()
    if args.source_dir:
        source_directory = args.source_dir
    update_knowledge(layer=args.layer)