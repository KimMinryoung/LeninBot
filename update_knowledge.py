import os
import glob
from dotenv import load_dotenv
from tqdm import tqdm
from supabase.client import Client, create_client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import torch

load_dotenv()

# 1. ì´ˆê¸°í™”
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# BGE-M3 ì„ë² ë”© ëª¨ë¸ (1024ì°¨ì›, ë‹¤êµ­ì–´ ì§€ì›)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"[System] Using device: {device}")

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': device},
    encode_kwargs={'normalize_embeddings': True}
)
vectorstore = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="lenin_corpus",
    query_name="match_documents",
)

source_directory = "./docs/lenin"
log_file = "processed_files.txt"

# 2. íŒŒì¼ í™•ì¥ìë³„ ë¡œë” ë§¤í•‘ (í´ë˜ìŠ¤ ìì²´ë¥¼ ë§¤í•‘)
LOADER_MAPPING = {
    ".pdf": PyPDFLoader,
    ".txt": TextLoader
}

def update_knowledge(layer="core_theory"):
    print(f"ğŸ“‚ {source_directory} í´ë”ì—ì„œ ìƒˆ ë¬¸ì„œë¥¼ íƒìƒ‰ ì¤‘... (layer: {layer})")
    
    # ë¡œê·¸ íŒŒì¼ì—ì„œ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì½ê¸°
    if os.path.exists(log_file):
        with open(log_file, "r", encoding="utf-8") as f:
            processed_files = set(f.read().splitlines())
    else:
        processed_files = set()

    # í•˜ìœ„ í´ë”ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë“  .txt, .pdf íŒŒì¼ ì°¾ê¸°
    all_files = []
    for ext in LOADER_MAPPING.keys():
        all_files.extend(glob.glob(os.path.join(source_directory, f"**/*{ext}"), recursive=True))

    # ê²½ë¡œë¥¼ í†µì¼í•˜ì—¬ ë¹„êµ (ì—­ìŠ¬ë˜ì‹œ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ os.path.normpath ì‚¬ìš©)
    new_files = [f for f in all_files if os.path.normpath(f) not in processed_files]

    if not new_files:
        print("âœ… ì¶”ê°€í•  ìƒˆ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_chunks = 0
    file_bar = tqdm(new_files, desc="íŒŒì¼ ì²˜ë¦¬", unit="file", position=0)
    for file_path in file_bar:
        file_name = os.path.basename(file_path)
        file_bar.set_postfix_str(file_name[:40])
        ext = os.path.splitext(file_name)[1].lower()

        try:
            # 3. ë‹¨ì¼ íŒŒì¼ ë¡œë” ì‹¤í–‰
            if ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                loader = LOADER_MAPPING[ext](file_path)

            docs = loader.load()

            # 4. í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            splits = text_splitter.split_documents(docs)

            # 4.5. ë©”íƒ€ë°ì´í„°ì— layer ë° source ì£¼ì…
            # íŒŒì¼ ë‚´ Title: í—¤ë”ê°€ ìˆìœ¼ë©´ ë¬¸í—Œ ì œëª©ì„ sourceë¡œ ì‚¬ìš©
            source_name = os.path.splitext(file_name)[0]
            try:
                with open(file_path, 'r', encoding='utf-8') as tf:
                    for line in tf:
                        line = line.strip()
                        if line.startswith("Title:") and line[6:].strip():
                            source_name = line[6:].strip()
                            break
                        if not line.startswith("Source:") and line:
                            break  # í—¤ë” ì˜ì—­ì„ ë²—ì–´ë‚˜ë©´ ì¤‘ë‹¨
            except Exception:
                pass
            for doc in splits:
                doc.metadata["layer"] = layer
                doc.metadata["source"] = source_name

            # 5. Supabase ì „ì†¡ (ë°°ì¹˜ ì²˜ë¦¬)
            batch_size = 5
            for i in range(0, len(splits), batch_size):
                vectorstore.add_documents(documents=splits[i:i+batch_size])
            total_chunks += len(splits)

            # 6. ë¡œê·¸ì— ì¶”ê°€ (ì„±ê³µ ì‹œì—ë§Œ)
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(os.path.normpath(file_path) + "\n")

        except Exception as e:
            tqdm.write(f"âŒ ì—ëŸ¬ ë°œìƒ ({file_name}): {e}")
    file_bar.close()

    print(f"\nâœ¨ ì§€ì‹ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì´ {len(new_files)}ê°œ ë¬¸ì„œ, {total_chunks}ê°œ ì²­í¬ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--layer", default="core_theory",
                        help="Metadata layer tag (e.g. core_theory, modern_analysis)")
    parser.add_argument("--source-dir",
                        help="Override source directory (default: ./docs/lenin)")
    args = parser.parse_args()
    if args.source_dir:
        source_directory = args.source_dir  # noqa: F841 - used by update_knowledge
    update_knowledge(layer=args.layer)