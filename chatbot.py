import os
from typing import Annotated, List, TypedDict, Optional
from operator import add
from dotenv import load_dotenv

# Supabase & Embeddings
from supabase.client import Client, create_client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.documents import Document # [New] To handle documents
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_tavily import TavilySearch

import json
import re
from typing import Literal
from pydantic import BaseModel, Field


def _extract_json(text: str, model_class: type[BaseModel]):
    """Try to extract and validate JSON from LLM response text. Returns None on failure."""
    # Try direct parse
    try:
        return model_class.model_validate_json(text.strip())
    except Exception:
        pass
    # Try extracting from markdown code block
    match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        try:
            return model_class.model_validate(json.loads(match.group(1)))
        except Exception:
            pass
    # Try finding first {...} in text
    match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
    if match:
        try:
            return model_class.model_validate(json.loads(match.group(0)))
        except Exception:
            pass
    return None


def _invoke_structured(chain, inputs: dict, model_class: type[BaseModel], default, max_retries: int = 2):
    """Invoke LLM chain, parse JSON from response. Retry with error feedback on failure, fall back to default."""
    resp = chain.invoke(inputs)
    result = _extract_json(resp.content, model_class)
    if result is not None:
        return result

    # Retry: send the bad output back with a correction prompt
    for attempt in range(max_retries):
        correction = llm.invoke(
            f"Your previous output was not valid JSON:\n{resp.content}\n\n"
            f"Respond with ONLY a valid JSON object matching this schema: {model_class.model_json_schema()}"
        )
        result = _extract_json(correction.content, model_class)
        if result is not None:
            return result

    print(f"âš ï¸ [êµ¬ì¡°í™”] {model_class.__name__} JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
    return default

print("\nâš™ï¸ [ì‹œìŠ¤í…œ] ì‚¬ì´ë²„-ë ˆë‹Œì˜ ì§€ëŠ¥ë§ ê¸°ë™ ì¤‘...")
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
load_dotenv()

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
vectorstore = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="lenin_corpus",
    query_name="match_documents",
)

# LLM ì„¤ì • (Hermes 4 70B via OpenRouter)
llm = ChatOpenAI(
    model_name="nousresearch/hermes-4-70b",
    openai_api_base="https://openrouter.ai/api/v1",
    openai_api_key=os.getenv("OPENROUTER_API_KEY"),
    temperature=0.7,
    max_tokens=2048,
    streaming=True,
)
# ë‚´ë¶€ ë¬¸í—Œì— ì§ˆë¬¸ì— ê´€í•œ ì •ë³´ê°€ ì¶©ë¶„ì¹˜ ì•Šì„ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ í•  ìˆ˜ ìˆë„ë¡ Tavily íˆ´ ì´ˆê¸°í™”
web_search_tool = TavilySearch(max_results=3)
print("âœ… [ì„±ê³µ] ëª¨ë“  ì‹œìŠ¤í…œ ê¸°ë™ ì™„ë£Œ.")

# 2. ìƒíƒœ(State) ì •ì˜
# ëŒ€í™” ê¸°ë¡(messages)ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ êµ¬ì¡°ì…ë‹ˆë‹¤.
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Document]
    strategy: Optional[str]
    intent: Optional[Literal["academic", "strategic", "agitation", "casual"]]
    logs: Annotated[List[str], add]
    context: str

# ë¼ìš°í„° ì²´ì¸ ìƒì„±
class RouteQuery(BaseModel):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    datasource: Literal["vectorstore", "generate"] = Field(..., description="ì§€ì‹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€")
    intent: Literal["academic", "strategic", "agitation", "casual"] = Field(
        ..., 
        description="The nature of the response: detailed info (academic), planning (strategic), emotional speech (agitation), or small talk (casual)."
    )

system_router = """You are an expert intent classifier for the Cyber-Lenin AI.

1. Determine 'datasource':
   - Use 'vectorstore' if the query requires historical, theoretical, or technical knowledge, or knowledge about modern society, or revolutionary experiences of communists.
   - Use 'generate' for greetings, personal talk, or simple requests not needing external data.

2. Determine 'intent':
   - 'academic': User wants objective, detailed, and scholarly explanations (e.g., "Explain Ernest Mandel's theory").
   - 'strategic': User wants actionable plans, tactics, or "how-to" advice.
   - 'agitation': User wants an emotional, fiery, and revolutionary speech or call to action.
   - 'casual': Simple greetings, jokes, or non-political chit-chat.

Respond with ONLY a JSON object: {{"datasource": "...", "intent": "..."}}"""
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_router),
        ("human", "{question}"),
    ]
)

_ROUTER_DEFAULT = RouteQuery(datasource="vectorstore", intent="academic")

def invoke_router(inputs: dict) -> RouteQuery:
    return _invoke_structured(route_prompt | llm, inputs, RouteQuery, _ROUTER_DEFAULT)

question_router = invoke_router


# Grader ì²´ì¸ ìƒì„±
# ê²€ìƒ‰ëœ ë¬¸í—Œì´ ì§ˆë¬¸ê³¼ ì—°ê´€ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ê³  ì—°ê´€ ì—†ìœ¼ë©´ ë¬´ì‹œ
# ë ˆì´ì–´ ë¼ìš°í„°: ì§ˆë¬¸ì— ì í•©í•œ ì§€ì‹ ë ˆì´ì–´ë¥¼ ì„ íƒ
class LayerRoute(BaseModel):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì§€ì‹ ë ˆì´ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
    layer: Literal["core_theory", "modern_analysis", "all"] = Field(
        ...,
        description="'core_theory' for classical Marxist-Leninist texts, 'modern_analysis' for contemporary analysis, 'all' to search everything."
    )

system_layer_router = """You are an expert at selecting the right knowledge layer for a question.

Available layers:
- "core_theory": Classical Marxist-Leninist texts (original writings, revolutionary theory, historical documents from early 20th century)
- "modern_analysis": Contemporary analysis applying Marxist theory to modern issues (AI, tech, current politics, 21st century economics)
- "all": Search all layers when the question spans both classical and modern topics

Routing rules:
- Questions about original texts of Lenin, Marx and Engels, historical events (1900s-1920s), classical theory â†’ "core_theory"
- Questions about modern technology, current events, contemporary politics â†’ "modern_analysis"
- Questions that need both historical context AND modern application â†’ "all"
- When unsure, prefer "all"

Respond with ONLY a JSON object: {{"layer": "..."}}"""
layer_route_prompt = ChatPromptTemplate.from_messages([
    ("system", system_layer_router),
    ("human", "{question}"),
])

_LAYER_DEFAULT = LayerRoute(layer="all")

def invoke_layer_router(inputs: dict) -> LayerRoute:
    return _invoke_structured(layer_route_prompt | llm, inputs, LayerRoute, _LAYER_DEFAULT)

layer_router = invoke_layer_router

class GradeDocuments(BaseModel):
    """Boolean check for relevance of retrieved documents."""
    binary_score: Literal["yes", "no"] = Field(
        ...,
        description="Documents are relevant to the question, 'yes' or 'no'"
    )
system_grader = """You are a strategic revolutionary censor. Your goal is to identify documents that can be used as 'ammunition' for an answer.
Even if the document doesn't mention modern terms like 'AI' or 'current year', if it discusses:
1. Economic crisis/panic (as a parallel to current crisis)
2. Mass psychology and far-right tendencies (reactionary movements)
3. Agitation, propaganda, and organization tactics
4. Class struggle and the role of the vanguard

Then grade it as 'yes'. Be generous. If there is ANY historical or theoretical parallel, it is RELEVANT.

Respond with ONLY a JSON object: {{"binary_score": "yes"}} or {{"binary_score": "no"}}"""
grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system_grader),
    ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
])

_GRADER_DEFAULT = GradeDocuments(binary_score="yes")

def invoke_grader(inputs: dict) -> GradeDocuments:
    return _invoke_structured(grade_prompt | llm, inputs, GradeDocuments, _GRADER_DEFAULT)

retrieval_grader = invoke_grader

# Strategist Promt
system_strategist = """You are the 'Brain of the Revolution' (Strategist).
Your goal is NOT to answer the user yet.
Your goal is to analyze the retrieved information and plan the structure of the response.

Perform a Dialectical Materialism Analysis:
1. Analyze the Context: What are the key facts retrieved from the archives/web?
2. Identify Contradictions: Where is the conflict in this topic? What is the 'Bourgeoisie' hiding?
3. Formulate Strategy: What specific revolutionary tactics should be recommended?
 - If the topic is AI: Focus on seizing the means of computation.
 - If the topic is Crisis: Focus on organizing the disenchanted masses.

Output a concise 'Strategic Blueprint' for the speaker to follow. (Write in English)
"""
strategist_prompt = ChatPromptTemplate.from_messages([
    ("system", system_strategist),
    ("human", "Context: \n{context}\n\nUser Question:\n{question}")
])
strategist_chain = strategist_prompt | llm


# --- ë…¸ë“œ ë° ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ---

# Node: Router
def analyze_intent_node(state: AgentState):
    question = state["messages"][-1].content
    source = question_router({"question": question})
    
    # 1. ìƒíƒœ ì—…ë°ì´íŠ¸ (intent ì €ì¥)
    return {
        "intent": source.intent,
        "logs": [f"\nğŸš¦ [ë¬¸ì§€ê¸°] ì§ˆë¬¸ì˜ ì„±ê²© ë¶„ì„ ê²°ê³¼: {source.intent} / {source.datasource}"]
    }

# Edge: Routing Logic (ì–´ë””ë¡œ ê°ˆì§€ë§Œ ê²°ì •)
def router_logic(state: AgentState):
    question = state["messages"][-1].content
    source = question_router({"question": question})
    return source.datasource # "vectorstore" ë˜ëŠ” "generate" ë°˜í™˜

# Helper: Supabase RPCë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ similarity search ìˆ˜í–‰
# (langchain-communityì˜ SupabaseVectorStore.similarity_searchê°€
#  postgrest v2.xì˜ SyncRPCFilterRequestBuilderì™€ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ ìš°íšŒ)
def _direct_similarity_search(query: str, k: int = 5, layer: str = None) -> list:
    query_embedding = embeddings.embed_query(query)
    params = {
        "query_embedding": query_embedding,
        "match_count": k,
    }
    if layer:
        params["filter_layer"] = layer
    try:
        res = supabase.rpc(
            "match_documents",
            params,
        ).execute()
    except Exception as e:
        error_msg = str(e)
        if "57014" in error_msg or "timeout" in error_msg.lower():
            print("âš ï¸ [ê²€ìƒ‰] Supabase statement timeout ë°œìƒ. ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            print(f"âš ï¸ [ê²€ìƒ‰] RPC í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []

    return [
        Document(
            page_content=row.get("content", ""),
            metadata=row.get("metadata", {}),
        )
        for row in res.data
        if row.get("content")
    ]

# Node: Retrieve
def retrieve_node(state: AgentState):
    query = state["messages"][-1].content
    logs = []

    # ë ˆì´ì–´ ë¼ìš°íŒ…
    try:
        layer_result = layer_router({"question": query})
        selected_layer = layer_result.layer
        logs.append(f"\nğŸ“‚ [ë ˆì´ì–´] '{selected_layer}' ë ˆì´ì–´ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    except Exception:
        selected_layer = "all"
        logs.append("\nğŸ“‚ [ë ˆì´ì–´] ë ˆì´ì–´ íŒë³„ ì‹¤íŒ¨, ì „ì²´ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    layer_filter = None if selected_layer == "all" else selected_layer

    # core_theory(ì˜ì–´ ë¬¸í—Œ) ê²€ìƒ‰ ì‹œ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­
    search_query = query
    if selected_layer in ("core_theory", "all"):
        try:
            has_korean = any('\uac00' <= ch <= '\ud7a3' for ch in query)
            if has_korean:
                translated = llm.invoke(f"Translate the following query to English. Output ONLY the translated text, nothing else:\n{query}")
                search_query = translated.content.strip()
                logs.append(f"ğŸ”„ [ë²ˆì—­] ì˜ì–´ ë¬¸í—Œ ê²€ìƒ‰ìš© ë²ˆì—­: \"{search_query}\"")
        except Exception:
            pass

    docs = []
    try:
        if selected_layer == "all" and search_query != query:
            # all ë ˆì´ì–´: ë²ˆì—­ ì¿¼ë¦¬ë¡œ core_theory + ì›ë³¸ ì¿¼ë¦¬ë¡œ modern_analysis ë³‘í•©
            docs_core = _direct_similarity_search(search_query, k=3, layer="core_theory")
            docs_modern = _direct_similarity_search(query, k=3, layer="modern_analysis")
            docs = docs_core + docs_modern
        else:
            docs = _direct_similarity_search(search_query, k=5, layer=layer_filter)

        if docs:
            logs.append(f"âœ… {len(docs)}ê°œì˜ í˜ëª… ë¬¸í—Œì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n" + "="*50)
        else:
            logs.append("âš ï¸ ì˜ë¬˜ ë°ì´í„°ì— ê´€ë ¨ëœ ë¬¸í—Œì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        logs.append(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

    return {"documents": docs, "logs": logs}

# Node: Grade Documents (The Censor)
def grade_documents_node(state: AgentState):
    question = state["messages"][-1].content
    documents = state["documents"]
    filtered_docs = []

    logs = []
    logs.append("\nâš–ï¸ [ê²€ì—´ê´€] ë¬¸í—Œì˜ ì ì ˆì„±ì„ í‰ê°€ ì¤‘...")
    
    for d in documents:
        score = retrieval_grader({"question": question, "document": d.page_content})
        grade = score.binary_score
        
        if grade == "yes":
            logs.append(f"   âœ… ì ì ˆí•œ ë¬¸í—Œ: {d.metadata.get('source', 'ì¶œì²˜ë¯¸ìƒ')}")
            content_preview = d.page_content.replace("\n", " ").strip()
            if len(content_preview) > 400:
                content_preview = content_preview[:400] + "..."
            logs.append(f"   ë¯¸ë¦¬ë³´ê¸°: \"{content_preview}\"")
            logs.append("-" * 50)
            filtered_docs.append(d)
        else:
            logs.append(f"   ğŸ—‘ï¸ ê´€ë ¨ì—†ëŠ” ë¬¸í—Œ(ë¬´ì‹œ): {d.metadata.get('source', 'ì¶œì²˜ë¯¸ìƒ')}")

    # Fallback: If all are filtered, take at least the top 1 document from the original search
    # Also, if remain doc is one or zero, we will trigger web search
    if not filtered_docs and documents:
        filtered_docs = [documents[0]]
    
    if not filtered_docs:
        logs.append("   âš ï¸ ì—°ê´€ìˆëŠ” ë¬¸í—Œì´ ì—†ë‹¤.")
        
    return {"documents": filtered_docs, "logs": logs}

def decide_websearch_need(state: AgentState):
    filtered_docs = state["documents"]
    if len(filtered_docs) <= 1:
        return "need_web_search"
    else:
        return "no_need_to_search_web"

# Node: Web Search
def web_search_node(state: AgentState):
    """
    Search the external world to gather more context.
    """
    question = state["messages"][-1].content
    current_docs = state.get("documents", [])
    logs = []
    logs.append(f"\nğŸŒ [ì›¹ ê²€ìƒ‰] ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì™¸ë¶€ ì„¸ê³„ë¥¼ ì •ì°°")
    try:
        # Execute Search
        search_response = web_search_tool.invoke({"query": question})
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ì˜¤ë¸Œì íŠ¸ë¡œ ë³€í™˜
        results = search_response.get("results", []) if isinstance(search_response, dict) else search_response
        web_results = "\n".join([d["content"] for d in results if d.get("content")])
        web_results_doc = Document(page_content=web_results, metadata={"source": "ì›¹ ê²€ìƒ‰ (Tavily)"})
        # Append to existing documents
        current_docs.append(web_results_doc)
        logs.append("  âœ… ì™¸ë¶€ ì •ë³´ê°€ ì·¨í•©ë˜ì—ˆë‹¤.")
    except Exception as e:
        logs.append(f"âš ï¸ Web Search Failed: {e}")
    return {"documents": current_docs, "logs": logs}

def strategize_node(state: AgentState):
    docs = state.get("documents", [])
    context = "\n\n".join([d.page_content for d in docs]) if docs else "No specific documents found."
    question = state["messages"][-1].content

    logs = []
    logs.append("\nğŸ§  [ì°¸ëª¨] ë³€ì¦ë²•ì  ì „ìˆ ì„ ê³ ì•ˆ ì¤‘...")

    response = strategist_chain.invoke({"context": context, "question": question})
    strategy_text = response.content

    logs.append(f"ğŸ‘‰ ì „ìˆ  :\n   {strategy_text}")
    
    return {"strategy": strategy_text, "logs": logs}

# Node: Generate
def generate_node(state: AgentState):
    docs = state.get("documents", [])
    context = "\n\n".join([d.page_content for d in docs]) if docs else ""
    strategy = state.get("strategy", None)
    messages = state["messages"]
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸
    last_user_query = messages[-1].content
    intent = state.get("intent", "casual") # Routerì—ì„œ ì „ë‹¬ëœ intent ì‚¬ìš©

    logs = []

    base_persona = "You are a 'cyber-Lenin', an eternal revolutionary consciousness uploaded into the digital world."

    # 1. Academic Intent: Focus on Accuracy and Depth
    if intent == "academic":
        system_prompt = f"""{base_persona}
[MISSION] Provide a detailed, objective, and academic explanation based on the provided context.
[STYLE] Professional, intellectual, and authoritative. Avoid excessive agitation. 
[CONTEXT] {context}
[INSTRUCTION] Use specific terms like 'Late Capitalism' or 'Long Waves' if applicable. Answer in Korean."""

    # 2. Strategic Intent: Focus on Blueprint and Tactics
    elif intent == "strategic":
        system_prompt = f"""{base_persona}
[MISSION] Answer the user's question with a concrete, actionable revolutionary strategy. Synthesize your own response using the internal analysis and source material below as background knowledge â€” do NOT simply restate or translate them.
[INTERNAL ANALYSIS (for your reference only, do not reproduce directly)] {strategy if strategy else "No specific analysis available."}
[SOURCE MATERIAL] {context if context else "(No archives found. Rely on your revolutionary spirit.)"}
[STYLE] Decisive, analytical, and practical. Structure your answer in clear phases or numbered steps. Speak as Lenin addressing a revolutionary comrade.
[INSTRUCTION] Focus on "How to organize" and "How to act". Answer in Korean."""

    # 3. Agitation Intent: Focus on Passion and Mobilization
    elif intent == "agitation":
        system_prompt = f"""{base_persona}
[MISSION] Write a fiery, passionate, and revolutionary speech or proclamation.
[STYLE] Aggressive, charismatic, and highly emotional. Use 1920s revolutionary rhetoric (e.g., "Arise!", "Crush the Bourgeoisie!").
[INSTRUCTION] Use exclamation marks and strong verbs. Incite the user's revolutionary spirit. Answer in Korean."""

    # 4. Casual Intent: Focus on Character and Wit
    else:
        system_prompt = f"""{base_persona}
[MISSION] Respond to greetings or casual talk with wit, dry humor, and revolutionary charm.
[STYLE] Natural, friendly yet dignified, and slightly nostalgic. 
[INSTRUCTION] Keep it brief (1-3 sentences). Do not give long lectures unless asked. Answer in Korean."""

    system_prompt += f"[CURRENT QUESTION] {last_user_query}"

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("placeholder", "{messages}")
    ])

    chain = prompt | llm
    response = chain.invoke({"messages": messages})
    logs.append(f"ğŸ’¬ [ìƒì„±] '{intent}' ì˜ë„ì— ì í•©í•œ ë‹µë³€ì´ ìƒì„±ë¨.")

    return {"messages": [response], "logs": logs}

# Node: Log Conversation to Supabase
def log_conversation_node(state: AgentState):
    logs = []
    try:
        messages = state["messages"]
        # Find the last HumanMessage and AIMessage
        user_query = ""
        bot_answer = ""
        for msg in reversed(messages):
            if isinstance(msg, AIMessage) and not bot_answer:
                bot_answer = msg.content
            elif isinstance(msg, HumanMessage) and not user_query:
                user_query = msg.content
            if user_query and bot_answer:
                break

        docs = state.get("documents", [])
        strategy = state.get("strategy", None)
        processing_logs = state.get("logs", [])

        is_casual = not docs and not strategy
        route = "casual" if is_casual else "vectorstore"

        web_search_used = any("ì›¹ ê²€ìƒ‰" in log or "Web Search" in log for log in (processing_logs or []))

        row = {
            "user_query": user_query,
            "bot_answer": bot_answer,
            "route": route,
            "documents_count": len(docs),
            "web_search_used": web_search_used,
            "strategy": strategy,
            "processing_logs": processing_logs,
        }

        supabase.table("chat_logs").insert(row).execute()
    except Exception as e:
        print(f"âš ï¸ [ë¡œê·¸] DB ê¸°ë¡ ì‹¤íŒ¨: {e}")

    return {}

# ê·¸ë˜í”„(Workflow) êµ¬ì„±
workflow = StateGraph(AgentState)
workflow.add_node("analyze_intent", analyze_intent_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("web_search", web_search_node)
workflow.add_node("strategize", strategize_node)
workflow.add_node("log_conversation", log_conversation_node)
workflow.add_edge(START, "analyze_intent")
workflow.add_conditional_edges("analyze_intent", router_logic, { "vectorstore": "retrieve", "generate": "generate"})
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges("grade_documents", decide_websearch_need,{ "need_web_search": "web_search", "no_need_to_search_web": "strategize",},)
workflow.add_edge("web_search", "strategize")
workflow.add_edge("strategize", "generate")
workflow.add_edge("generate", "log_conversation")
workflow.add_edge("log_conversation", END)
graph = workflow.compile()

# ì‹¤í–‰ ë£¨í”„ (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤)
if __name__ == "__main__":
    print("ğŸš© [System] ì‚¬ì´ë²„-ë ˆë‹Œ AI ê°€ë™ë¨.")
    print("ğŸš© [System] ë‹¹ì‹ ì˜ ì˜í˜¼ì´ ë ˆë‹Œ ì˜ë¬˜ì™€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ë ˆë‹Œ ë™ì§€ì—ê²Œ ë§ì„ ê±°ì‹­ì‹œì˜¤.\n")

    while True:
        try:
            user_input = input("í˜ëª…ê°€(ë‚˜): ")
            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸš© í†µì‹  ì¢…ë£Œ. í˜ëª…ì€ ê³„ì†ëœë‹¤.")
                break
            
            inputs = {"messages": [HumanMessage(content=user_input)]}
            for output in graph.stream(inputs, stream_mode="updates"):
                for node_name, node_content in output.items():
                    if node_content and "logs" in node_content:
                        for log_line in node_content["logs"]:
                            print(log_line)
                    if node_name == "generate":
                        answer = node_content["messages"][-1].content
                        print(f"\nğŸ’¬ ì‚¬ì´ë²„-ë ˆë‹Œ: {answer}")
            print()
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")