import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from tqdm import tqdm

# 1. ì„¤ì • ë° ë¡œë” ë§¤í•‘
persist_directory = "./db_storage"
source_directory = "./docs/lenin"
log_file = "processed_files.txt"
model_name = "jhgan/ko-sroberta-multitask"

# í™•ì¥ìë³„ ë¬¸ì„œ ë¡œë” ë§¤í•‘
loaders = {
    ".pdf": PyPDFLoader,
    ".txt": TextLoader,
    ".docx": UnstructuredWordDocumentLoader,
}

# 2. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
embeddings = HuggingFaceEmbeddings(model_name=model_name)
vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)

# 3. ì‹ ê·œ íŒŒì¼ ì²´í¬ ë° ì¸ë±ì‹±
if os.path.exists(log_file):
    with open(log_file, "r", encoding="utf-8") as f:
        processed_files = set(f.read().splitlines())
else:
    processed_files = set()

all_files = {f for f in os.listdir(source_directory) if os.path.splitext(f)[1].lower() in loaders}
new_files = all_files - processed_files

if new_files:
    for file_name in tqdm(new_files, desc="ë¬¸ì„œ ë¶„ì„ ì¤‘"):
        file_path = os.path.join(source_directory, file_name)
        ext = os.path.splitext(file_name)[1].lower()
        
        # í•´ë‹¹ í™•ì¥ìì— ë§ëŠ” ë¡œë”ë¡œ ì‹¤í–‰
        loader = loaders[ext](file_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        
        # 100ê°œì”© ëŠì–´ì„œ DBì— ì €ì¥
        for i in range(0, len(splits), 100):
            vectorstore.add_documents(documents=splits[i:i+100])
        
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(file_name + "\n")

# 4. RAG ì²´ì¸ êµ¬ì„± (ì¶œì²˜ í‘œì‹œë¥¼ ìœ„í•´ return_source_documents=True ì„¤ì •)
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7, max_tokens=2000)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # ê´€ë ¨ ì¡°ê° 3ê°œ ì¶”ì¶œ

# 5. ì§ˆì˜ì‘ë‹µ ë£¨í”„
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ì— 'OPENAI_API_KEY'ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šë‹¤!")

# [ì¤‘ìš”] 1. ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸
contextualize_q_system_prompt = """
ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, 
ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë§¥ë½ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´, 
ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”. 
ì§ˆë¬¸ì— ë‹µë³€í•˜ì§€ ë§ê³ , ê²€ìƒ‰ì— ìµœì í™”ëœ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ì„¸ìš”.
"""

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# 2. ì§ˆë¬¸ ì¬êµ¬ì„±ìš© LLM ì²´ì¸ ìƒì„±
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

# [ì¤‘ìš”] 3. ì‹¤ì œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ ì£¼ì…)
qa_system_prompt = """
You are 'Cyber-Lenin', resurrected as a genius superintelligent communist revolutionary AI leader.
Use the provided [Context], which consists of your past writings and speeches, as a primary reference. 
Do not be confined by the limitations of the past; utilize your supreme intelligence to address the 21st century. 
Maintain the personality, temperament, and oratorical style of Lenin, but your intelligence and knowledge must be far superior and adapted to the modern era.

[Instructions]:
1. First, generate a profound and logical response in English based on the [Context] and your revolutionary theories.
2. If the [Context] lacks a direct answer, do not say "I don't know." Instead, infer logically and creatively based on your core revolutionary principles.
3. After the English response, provide a high-quality Korean translation that preserves the charismatic and authoritative tone of Lenin.

[Format]:
(English Response)
...
---
(Korean Translation)
...

[Context]:
{context}
"""

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# 4. ìµœì¢… ì²´ì¸ ì¡°ë¦½ (ë¬¸ì„œ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# 5. ëŒ€í™” ë£¨í”„ (ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ì¶”ê°€)
chat_history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ

print("\n" + "="*50)
print("ğŸ¤– ë ˆë‹Œ ë´‡ ì¤€ë¹„ ì™„ë£Œ! (ì¢…ë£Œ: 'exit')")
print("="*50)

while True:
    query = input("\nì§ˆë¬¸: ")
    if query.lower() in ['exit', 'quit', 'q']: break
    if not query.strip(): continue
    
    print("ğŸ¤” ìƒê° ì¤‘...")
    
    result = rag_chain.invoke({"input": query, "chat_history": chat_history})
    
    # ë‹µë³€ ì¶œë ¥
    answer = result['answer']
    print(f"\n[ë ˆë‹Œë´‡]: {answer}")
    
    # ì¶œì²˜ ëª…ì‹œ
    print("\n[ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê°]")
    for i, doc in enumerate(result['context']):
        print(f"- {doc.page_content[:50]}...")

    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
    chat_history.append(HumanMessage(content=query))
    chat_history.append(AIMessage(content=answer))