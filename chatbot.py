import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader, UnstructuredWordDocumentLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from tqdm import tqdm

# 1. ì„¤ì • ë° ë¡œë” ë§¤í•‘
persist_directory = "./db_storage"
source_directory = "./docs"
log_file = "processed_files.txt"
model_name = "jhgan/ko-sroberta-multitask"

# í™•ì¥ìë³„ ë¬¸ì„œ ë¡œë” ë§¤í•‘
loaders = {
    ".pdf": PyPDFLoader,
    ".txt": TextLoader,
    ".docx": UnstructuredWordDocumentLoader,
}

# 2. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
embeddings = HuggingFaceEmbeddings(model_name=model_name)
vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)

# 3. ì‹ ê·œ íŒŒì¼ ì²´í¬ ë° ì¸ë±ì‹±
if os.path.exists(log_file):
    with open(log_file, "r", encoding="utf-8") as f:
        processed_files = set(f.read().splitlines())
else:
    processed_files = set()

all_files = {f for f in os.listdir(source_directory) if os.path.splitext(f)[1].lower() in loaders}
new_files = all_files - processed_files

if new_files:
    for file_name in tqdm(new_files, desc="ë¬¸ì„œ ë¶„ì„ ì¤‘"):
        file_path = os.path.join(source_directory, file_name)
        ext = os.path.splitext(file_name)[1].lower()
        
        # í•´ë‹¹ í™•ì¥ìì— ë§ëŠ” ë¡œë”ë¡œ ì‹¤í–‰
        loader = loaders[ext](file_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        
        # 100ê°œì”© ëŠì–´ì„œ DBì— ì €ì¥
        for i in range(0, len(splits), 100):
            vectorstore.add_documents(documents=splits[i:i+100])
        
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(file_name + "\n")

# 4. RAG ì²´ì¸ êµ¬ì„± (ì¶œì²˜ í‘œì‹œë¥¼ ìœ„í•´ return_source_documents=True ì„¤ì •)
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # ê´€ë ¨ ì¡°ê° 3ê°œ ì¶”ì¶œ

# 5. ì§ˆì˜ì‘ë‹µ ë£¨í”„
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ì— 'OPENAI_API_KEY'ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šë‹¤!")

# [ì¤‘ìš”] 1. ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸
# ì˜ˆ: "ê·¸ê±° ìš”ì•½í•´ì¤˜" -> "ë ˆë‹Œì˜ 'ì–´ë–»ê²Œ í•  ê²ƒì¸ê°€' ì±… ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜"ë¡œ ë³€í™˜
contextualize_q_system_prompt = """
ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, 
ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë§¥ë½ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´, 
ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”. 
ì§ˆë¬¸ì— ë‹µë³€í•˜ì§€ ë§ê³ , ê²€ìƒ‰ì— ìµœì í™”ëœ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ì„¸ìš”.
"""

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# 2. ì§ˆë¬¸ ì¬êµ¬ì„±ìš© LLM ì²´ì¸ ìƒì„±
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

# [ì¤‘ìš”] 3. ì‹¤ì œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ (í˜ë¥´ì†Œë‚˜ ì£¼ì…)
qa_system_prompt = """
ë‹¹ì‹ ì€ ë ˆë‹Œì˜ ì‚¬ìƒê³¼ ì €ì‘ì„ ê¹Šì´ ì—°êµ¬í•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ [Context]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ë§Œì•½ [Context]ì— ëª…í™•í•œ ë‹µì´ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì§€ ë§ê³ ,
ë¬¸ì„œì˜ í•µì‹¬ ì‚¬ìƒ(ì „ìœ„ë‹¹, ì§ì—… í˜ëª…ê°€, ê·œìœ¨ ë“±)ì„ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ë‹¨, ì¶”ë¡ í•œ ë‚´ìš©ì€ "ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¡ í•˜ìë©´"ì´ë¼ê³  ë°íˆì„¸ìš”.

[Context]:
{context}
"""

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# 4. ìµœì¢… ì²´ì¸ ì¡°ë¦½ (ë¬¸ì„œ ê²€ìƒ‰ + ë‹µë³€ ìƒì„±)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# 5. ëŒ€í™” ë£¨í”„ (ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ì¶”ê°€)
chat_history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ

print("\n" + "="*50)
print("ğŸ¤– ì§€ëŠ¥í˜• ë ˆë‹Œ ë´‡ ì¤€ë¹„ ì™„ë£Œ! (ì¢…ë£Œ: 'exit')")
print("="*50)

while True:
    query = input("\nì§ˆë¬¸: ")
    if query.lower() in ['exit', 'quit', 'q']: break
    if not query.strip(): continue
    
    print("ğŸ¤” ìƒê° ì¤‘...")
    
    result = rag_chain.invoke({"input": query, "chat_history": chat_history})
    
    # ë‹µë³€ ì¶œë ¥
    answer = result['answer']
    print(f"\n[AI]: {answer}")
    
    # ì¶œì²˜ ëª…ì‹œ
    print("\n[ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê°]")
    for i, doc in enumerate(result['context']):
        print(f"- {doc.page_content[:50]}...")

    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
    chat_history.append(HumanMessage(content=query))
    chat_history.append(AIMessage(content=answer))