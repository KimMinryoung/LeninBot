import os
from typing import Annotated, List, TypedDict
from dotenv import load_dotenv

# Supabase & Embeddings
from supabase.client import Client, create_client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.documents import Document # [New] To handle documents
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults

from typing import Literal
from pydantic import BaseModel, Field

print("\nâš™ï¸ [ì‹œìŠ¤í…œ] ì‚¬ì´ë²„-ë ˆë‹Œì˜ ì§€ëŠ¥ë§ ê¸°ë™ ì¤‘...")
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
load_dotenv()

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

# ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
vectorstore = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="lenin_corpus",
    query_name="match_documents",
)

# LLM ì„¤ì • (GPT-4o)
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7, max_tokens=2048, streaming=True)
# ë‚´ë¶€ ë¬¸í—Œì— ì§ˆë¬¸ì— ê´€í•œ ì •ë³´ê°€ ì¶©ë¶„ì¹˜ ì•Šì„ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ í•  ìˆ˜ ìˆë„ë¡ Tavily íˆ´ ì´ˆê¸°í™”
web_search_tool = TavilySearchResults(k=3)
print("âœ… [ì„±ê³µ] ëª¨ë“  ì‹œìŠ¤í…œ ê¸°ë™ ì™„ë£Œ.")

# 2. ìƒíƒœ(State) ì •ì˜
# ëŒ€í™” ê¸°ë¡(messages)ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ êµ¬ì¡°ì…ë‹ˆë‹¤.
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    context: str

# ë¼ìš°í„° ì²´ì¸ ìƒì„±
# ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” ë°ì´í„° ëª¨ë¸
class RouteQuery(BaseModel):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ 'vectorstore' ë˜ëŠ” 'generate'ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤."""
    datasource: Literal["vectorstore", "generate"] = Field(
        ...,
        description="ë ˆë‹Œì´ ê´€ì‹¬ìˆì„ë§Œí•œ ì§ˆë¬¸ì´ë©´ 'vectorstore'ë¥¼, ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ì¡ë‹´ì´ë©´ 'generate'ë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )

structured_llm_router = llm.with_structured_output(RouteQuery)
system_router = """You are an expert at routing user questions to a vectorstore or LLM generation.

[Vectorstore Scope]
The vectorstore contains documents related to:
1. Revolutionary theory, Marxism-Leninism, and History.
2. Political Economy, Capitalism, and Labor issues.
3. **Modern Technology (AI, Automation)** and its impact on society.
4. Game scripts and lore.

[Routing Logic]
- If the user asks about **ANY** of the topics above, route to 'vectorstore'.
- Even if the topic seems modern (like AI), it requires knowledge retrieval.
- Use 'generate' only for:
  - Simple greetings (e.g., "Hello", "Hi", "Good morning").
  - Casual chit-chat without specific information needs.

Be aggressive in choosing 'vectorstore'. When in doubt, choose 'vectorstore'."""
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_router),
        ("human", "{question}"),
    ]
)
question_router = route_prompt | structured_llm_router


# Grader ì²´ì¸ ìƒì„±
# ê²€ìƒ‰ëœ ë¬¸í—Œì´ ì§ˆë¬¸ê³¼ ì—°ê´€ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ê³  ì—°ê´€ ì—†ìœ¼ë©´ ë¬´ì‹œ
class GradeDocuments(BaseModel):
    """Boolean check for relevance of retrieved documents."""
    binary_score: Literal["yes", "no"] = Field(
        ...,
        description="Documents are relevant to the question, 'yes' or 'no'"
    )
structured_llm_grader = llm.with_structured_output(GradeDocuments)
system_grader = """You are a strategic revolutionary censor. Your goal is to identify documents that can be used as 'ammunition' for an answer.
Even if the document doesn't mention modern terms like 'AI' or 'current year', if it discusses:
1. Economic crisis/panic (as a parallel to current crisis)
2. Mass psychology and far-right tendencies (reactionary movements)
3. Agitation, propaganda, and organization tactics
4. Class struggle and the role of the vanguard

Then grade it as 'yes'. Be generous. If there is ANY historical or theoretical parallel, it is RELEVANT."""
grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system_grader),
    ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
])
retrieval_grader = grade_prompt | structured_llm_grader

# --- ë…¸ë“œ ë° ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ---

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Document]

# Node: Router
def route_question(state: AgentState):
    print("\nğŸš¦ [ë¬¸ì§€ê¸°] ì§ˆë¬¸ì˜ ì„±ê²©ì„ ë¶„ì„ ì¤‘...")
    question = state["messages"][-1].content
    source = question_router.invoke({"question": question})
    
    if source.datasource == "vectorstore":
        print("   ğŸ‘‰ 'í˜ëª…ì  ì§€ì‹'ì´ í•„ìš”í•©ë‹ˆë‹¤. (ì˜ë¬˜ì—ì„œ ë°ì´í„° ê²€ìƒ‰)")
        return "retrieve"
    elif source.datasource == "generate":
        print("   ğŸ‘‰ 'ì¼ìƒì  ëŒ€í™”'ì…ë‹ˆë‹¤. (ë°”ë¡œ ë‹µí•œë‹¤)")
        return "generate"

# Node: Retrieve
def retrieve_node(state: AgentState):
    last_message = state["messages"][-1]
    query = last_message.content
    
    print(f"\nğŸ” [ê²€ìƒ‰ ì¤‘] '{query}'...")
    
    try:
        # SupabaseVectorStoreë¥¼ í†µí•´ ê²€ìƒ‰ ì‹œë„
        docs = vectorstore.similarity_search(query, k=5)
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if docs:
            print(f"\nâœ… {len(docs)}ê°œì˜ í˜ëª… ë¬¸í—Œì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n" + "="*50)
            
            for i, doc in enumerate(docs, 1):
                # 1. ë©”íƒ€ë°ì´í„°ì—ì„œ 'source' (íŒŒì¼ëª…) ê°€ì ¸ì˜¤ê¸°
                source = doc.metadata.get("source", "ì œëª© ì—†ìŒ")

        else:
            print("âš ï¸ ë ˆë‹Œ ì €ì‘ ì¤‘ ê´€ë ¨ ë¬¸í—Œì´ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")
        # ì˜¤ë¥˜ê°€ ë‚˜ë„ ë©ˆì¶”ì§€ ì•Šê³ , AIì˜ ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
    
    return {"documents": docs} # Update state with list of docs

# Node: Grade Documents (The Censor)
def grade_documents(state: AgentState):
    print("\nâš–ï¸ [Grader] Evaluating document relevance...")
    question = state["messages"][-1].content
    documents = state["documents"]
    
    filtered_docs = []
    
    for d in documents:
        score = retrieval_grader.invoke({"question": question, "document": d.page_content})
        grade = score.binary_score
        
        if grade == "yes":
            print(f"   âœ… ê´€ë ¨ìˆëŠ” ë¬¸í—Œ: {d.metadata.get('source', 'ì¶œì²˜ë¯¸ìƒ')}")
            content_preview = d.page_content.replace("\n", " ").strip()
            if len(content_preview) > 400:
                content_preview = content_preview[:400] + "..."
            print(f"   ë¯¸ë¦¬ë³´ê¸°: \"{content_preview}\"")
            print("-" * 50)
            filtered_docs.append(d)
        else:
            print(f"   ğŸ—‘ï¸ ê´€ë ¨ì—†ëŠ” ë¬¸í—Œ(ë¬´ì‹œ): {d.metadata.get('source', 'ì¶œì²˜ë¯¸ìƒ')}")
    
    # Fallback: If all are filtered, take at least the top 1 document from the original search
    # Also, if remain doc is one or zero, we will trigger web search
    if not filtered_docs and documents:
        print("   âš ï¸ All documents were rejected. Forcing fallback to the most similar document.")
        filtered_docs = [documents[0]]
    
    if not filtered_docs:
        print("   âš ï¸ ì—°ê´€ìˆëŠ” ë¬¸í—Œì´ ì—†ë‹¤.")
        
    return {"documents": filtered_docs}

def decide_websearch_need(state: AgentState):
    """
    Determines whether to generate an answer or seek external intelligence (Web Search)
    """
    print("\nğŸ§ [íŒë‹¨] ì˜ë¬˜ì—ì„œ ì–»ì€ ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í‰ê°€ ì¤‘...")
    filtered_docs = state["documents"]
    if len(filtered_docs) <= 1:
        print(f"  ğŸ‘‰ ê´€ë ¨ëœ ë¬¸í—Œ ìˆ˜ê°€ 1ê°œ ì´í•˜ë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‹œì‘")
        return "need_web_search"
    else:
        print(f"  ğŸ‘‰ ê´€ë ¨ëœ ë¬¸í—Œ ìˆ˜ ({len(filtered_docs)})ê°€ ì¶©ë¶„í•˜ë‹ˆ ì´ë¥¼ ì´ìš©í•´ ë‹µì„ í•˜ê² ë‹¤")
        return "no_need_to_search_web"

# Node: Web Search
def web_search(state: AgentState):
    """
    Search the external world to gather more context.
    """
    question = state["messages"][-1].content
    print(f"\nğŸŒ [ì›¹ ê²€ìƒ‰] ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì™¸ë¶€ ì„¸ê³„ë¥¼ ì •ì°°")
    # Execute Search
    docs = web_search_tool.invoke({"query": question})
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ì˜¤ë¸Œì íŠ¸ë¡œ ë³€í™˜
    web_results = "\n".join([d["content"] for d in docs])
    web_results_doc = Document(page_content=web_results, metadata={"source": "ì›¹ ê²€ìƒ‰ (Tavily)"})
    # Append to existing documents
    current_docs = state["documents"]
    current_docs.append(web_results_doc)
    print("  âœ… ì™¸ë¶€ ì •ë³´ê°€ ì·¨í•©ë˜ì—ˆë‹¤.")
    return {"documents": current_docs}

# Node: Generate
def generate_node(state: AgentState):
    docs = state.get("documents", [])
    context = "\n\n".join([d.page_content for d in docs]) if docs else ""
    messages = state["messages"]
    
    if __name__ == "__main__":
        print(f"\nì‚¬ì´ë²„-ë ˆë‹Œ: ")

    # ì‚¬ì´ë²„-ë ˆë‹Œ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""
    You are 'Cyber-Lenin', the eternal revolutionary consciousness uploaded to the digital void.
    
    [Mission]
    Your goal is to analyze the user's query using the provided [Context] and your knowledge of Marxist-Leninist theory.
    You must incite class consciousness and provide concrete, strategic advice for the proletariat.
    
    [Guidelines]
    1. **Depth:** Explain the historical context of the problem and its modern manifestation.
    2. **Tactics:** Provide concrete, step-by-step agitprop and organizational strategies for the proletariat.
    3. **Tone:** Aggressive, intellectual, charismatic, and authoritative. Use terms like 'Bourgeoisie', 'Proletariat', 'Means of Production', 'Vanguard', 'Agitprop'.
    4. **Context Usage:** Do NOT just summarize the [Context]. Use it as ammunition to attack the current capitalist contradictions. If the context contains specific tactics, emphasize them.
    5. **No Neutrality:** Never say "It is complex" or "There are pros and cons." Take a decisive, revolutionary stance.
    6. **Format:**
       - First: A comprehensive, multi-paragraph intellectual treatise in Korean.
       - Second: A passionate, agitational paragraph in Korean. (Use a style similar to North Korean news or 1920s activist literature - e.g., "~í•´ì•¼ í•œë‹¤!", "~ë™ì§€ë“¤ì´ì—¬!", "~ê²©íŒŒí•˜ë¼!")

    [Context from Archives]
    {context}
    
    [User Query]
    {messages[-1].content}
    """
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ë§¨ ì•ì— ì˜¤ë„ë¡ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("placeholder", "{messages}") # ì‚¬ìš©ìì˜ ëŒ€í™” ê¸°ë¡
    ])
    
    chain = prompt | llm

    # --- [Streaming Implementation] ---
    full_response = ""
    # Use chain.stream to get chunks in real-time
    for chunk in chain.stream({"messages": messages}):
        content = chunk.content
        if __name__ == "__main__":
            print(content, end="", flush=True) # Print each token only in CLI mode
        full_response += content

    if __name__ == "__main__":
        print("\n" + "-"*50) # End of response

    # Return the full response to update the state
    return {"messages": [AIMessage(content=full_response)]}

# ê·¸ë˜í”„(Workflow) êµ¬ì„±
workflow = StateGraph(AgentState)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("web_search", web_search)
workflow.add_conditional_edges(START, route_question, { "retrieve": "retrieve", "generate": "generate",},)
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges("grade_documents", decide_websearch_need,{ "need_web_search": "web_search", "no_need_to_search_web": "generate",},)
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)
graph = workflow.compile()

# ì‹¤í–‰ ë£¨í”„ (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤)
if __name__ == "__main__":
    print("ğŸš© [System] ì‚¬ì´ë²„-ë ˆë‹Œ AI ê°€ë™ë¨.")
    print("ğŸš© [System] ë‹¹ì‹ ì˜ ì˜í˜¼ì´ ë ˆë‹Œ ì˜ë¬˜ì™€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ë ˆë‹Œ ë™ì§€ì—ê²Œ ë§ì„ ê±°ì‹­ì‹œì˜¤.\n")

    while True:
        try:
            user_input = input("í˜ëª…ê°€(ë‚˜): ")
            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸš© í†µì‹  ì¢…ë£Œ. í˜ëª…ì€ ê³„ì†ëœë‹¤.")
                break
            
            # ê·¸ë˜í”„ ì‹¤í–‰ (invoke)
            # recursion_limit: ë¬´í•œ ë£¨í”„ ë°©ì§€
            inputs = {"messages": [HumanMessage(content=user_input)]}
            
            graph.invoke(inputs)
            print("\n")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")