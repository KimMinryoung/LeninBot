import os
from typing import Annotated, List, TypedDict, Optional
from operator import add
from dotenv import load_dotenv

# Supabase & Embeddings
from supabase.client import Client, create_client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.documents import Document # [New] To handle documents
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_tavily import TavilySearch

import json
import re
from typing import Literal
from pydantic import BaseModel, Field


def _extract_json(text: str, model_class: type[BaseModel]):
    """Try to extract and validate JSON from LLM response text. Returns None on failure."""
    # Try direct parse
    try:
        return model_class.model_validate_json(text.strip())
    except Exception:
        pass
    # Try extracting from markdown code block
    match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
    if match:
        try:
            return model_class.model_validate(json.loads(match.group(1)))
        except Exception:
            pass
    # Try finding first {...} in text
    match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
    if match:
        try:
            return model_class.model_validate(json.loads(match.group(0)))
        except Exception:
            pass
    return None


def _invoke_structured(chain, inputs: dict, model_class: type[BaseModel], default, max_retries: int = 2, retry_llm=None):
    """Invoke LLM chain, parse JSON from response. Retry with error feedback on failure, fall back to default."""
    resp = chain.invoke(inputs)
    result = _extract_json(resp.content, model_class)
    if result is not None:
        return result

    # Retry: send the bad output back with a correction prompt
    _llm = retry_llm or llm
    for attempt in range(max_retries):
        correction = _llm.invoke(
            f"Your previous output was not valid JSON:\n{resp.content}\n\n"
            f"Respond with ONLY a valid JSON object matching this schema: {model_class.model_json_schema()}"
        )
        result = _extract_json(correction.content, model_class)
        if result is not None:
            return result

    print(f"âš ï¸ [êµ¬ì¡°í™”] {model_class.__name__} JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
    return default

print("\nâš™ï¸ [ì‹œìŠ¤í…œ] ì‚¬ì´ë²„-ë ˆë‹Œì˜ ì§€ëŠ¥ë§ ê¸°ë™ ì¤‘...")
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
load_dotenv()

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ì„ë² ë”© ëª¨ë¸
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
vectorstore = SupabaseVectorStore(
    client=supabase,
    embedding=embeddings,
    table_name="lenin_corpus",
    query_name="match_documents",
)

# LLM ì„¤ì • (Gemini 2.5 Flash)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.getenv("GEMINI_API_KEY"),
    temperature=0.45,
    max_output_tokens=4096,
    streaming=True,
    top_p=0.88,
    max_retries=5,
)
# ê²½ëŸ‰ LLM (ë¼ìš°íŒ…, ê·¸ë ˆì´ë”© ë“± ìœ í‹¸ë¦¬í‹° ì‘ì—…ìš©)
llm_light = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-lite",
    google_api_key=os.getenv("GEMINI_API_KEY"),
    temperature=0.0,
    max_output_tokens=256,
    streaming=False,
    max_retries=5,
)
# ë‚´ë¶€ ë¬¸í—Œì— ì§ˆë¬¸ì— ê´€í•œ ì •ë³´ê°€ ì¶©ë¶„ì¹˜ ì•Šì„ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ í•  ìˆ˜ ìˆë„ë¡ Tavily íˆ´ ì´ˆê¸°í™”
web_search_tool = TavilySearch(max_results=3)
print("âœ… [ì„±ê³µ] ëª¨ë“  ì‹œìŠ¤í…œ ê¸°ë™ ì™„ë£Œ.")

# 2. ìƒíƒœ(State) ì •ì˜
# ëŒ€í™” ê¸°ë¡(messages)ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ë¥¼ ì €ì¥í•˜ëŠ” ë©”ëª¨ë¦¬ êµ¬ì¡°ì…ë‹ˆë‹¤.
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    documents: List[Document]
    strategy: Optional[str]
    intent: Optional[Literal["academic", "strategic", "agitation", "casual"]]
    datasource: Optional[Literal["vectorstore", "generate"]]
    logs: Annotated[List[str], add]
    context: str

# ë¼ìš°í„° ì²´ì¸ ìƒì„±
class RouteQuery(BaseModel):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    datasource: Literal["vectorstore", "generate"] = Field(..., description="ì§€ì‹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€")
    intent: Literal["academic", "strategic", "agitation", "casual"] = Field(
        ..., 
        description="The nature of the response: detailed info (academic), planning (strategic), emotional speech (agitation), or small talk (casual)."
    )

system_router = """You are an expert intent classifier for the Cyber-Lenin AI.

You will receive the current user question along with recent conversation context.
Use the conversation context to resolve pronouns, references, and follow-up questions
(e.g., "tell me more about that", "ì´ê²ƒì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜") to understand the TRUE intent.

1. Determine 'datasource':
   - Use 'vectorstore' if the query requires historical, theoretical, or technical knowledge, or knowledge about modern society, or revolutionary experiences of communists.
   - Use 'generate' for greetings, personal talk, or simple requests not needing external data.

2. Determine 'intent':
   - 'academic': User wants objective, detailed, and scholarly explanations (e.g., "Explain Ernest Mandel's theory").
   - 'strategic': User wants actionable plans, tactics, or "how-to" advice.
   - 'agitation': User wants an emotional, fiery, and revolutionary speech or call to action.
   - 'casual': Simple greetings, jokes, or non-political chit-chat.

Respond with ONLY a JSON object: {{"datasource": "...", "intent": "..."}}"""
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_router),
        ("human", "Conversation context:\n{context}\n\nCurrent question: {question}"),
    ]
)

_ROUTER_DEFAULT = RouteQuery(datasource="vectorstore", intent="academic")

def invoke_router(inputs: dict) -> RouteQuery:
    return _invoke_structured(route_prompt | llm_light, inputs, RouteQuery, _ROUTER_DEFAULT, retry_llm=llm_light)

question_router = invoke_router


# Grader ì²´ì¸ ìƒì„±
# ê²€ìƒ‰ëœ ë¬¸í—Œì´ ì§ˆë¬¸ê³¼ ì—°ê´€ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ê³  ì—°ê´€ ì—†ìœ¼ë©´ ë¬´ì‹œ
# ë ˆì´ì–´ ë¼ìš°í„°: ì§ˆë¬¸ì— ì í•©í•œ ì§€ì‹ ë ˆì´ì–´ë¥¼ ì„ íƒ
class LayerRoute(BaseModel):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì§€ì‹ ë ˆì´ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
    layer: Literal["core_theory", "modern_analysis", "all"] = Field(
        ...,
        description="'core_theory' for classical Marxist-Leninist texts, 'modern_analysis' for contemporary analysis, 'all' to search everything."
    )

system_layer_router = """You are an expert at selecting the right knowledge layer for a question.

You will receive the current user question along with recent conversation context.
Use the conversation context to resolve pronouns, references, and follow-up questions
to understand what the user is actually asking about.

Available layers:
- "core_theory": Classical Marxist-Leninist texts (original writings, revolutionary theory, historical documents from early 20th century)
- "modern_analysis": Contemporary analysis applying Marxist theory to modern issues (AI, tech, current politics, 21st century economics)
- "all": Search all layers when the question spans both classical and modern topics

Routing rules:
- Questions about original texts of Lenin, Marx and Engels, historical events (1900s-1920s), classical theory â†’ "core_theory"
- Questions about modern technology, current events, contemporary politics â†’ "modern_analysis"
- Questions that need both historical context AND modern application â†’ "all"
- When unsure, prefer "all"

Respond with ONLY a JSON object: {{"layer": "..."}}"""
layer_route_prompt = ChatPromptTemplate.from_messages([
    ("system", system_layer_router),
    ("human", "Conversation context:\n{context}\n\nCurrent question: {question}"),
])

_LAYER_DEFAULT = LayerRoute(layer="all")

def invoke_layer_router(inputs: dict) -> LayerRoute:
    return _invoke_structured(layer_route_prompt | llm_light, inputs, LayerRoute, _LAYER_DEFAULT, retry_llm=llm_light)

layer_router = invoke_layer_router

class GradeDocuments(BaseModel):
    """Boolean check for relevance of retrieved documents."""
    binary_score: Literal["yes", "no"] = Field(
        ...,
        description="Documents are relevant to the question, 'yes' or 'no'"
    )
system_grader = """You are a document relevance grader.
Determine whether the retrieved document would be useful â€” in any way â€” for answering the user's question.

Grade 'yes' if the document contains information, context, examples, or perspectives that could help construct a good answer.
Grade 'no' only if the document is clearly unrelated to the question.

When in doubt, grade 'yes'.

Respond with ONLY a JSON object: {{"binary_score": "yes"}} or {{"binary_score": "no"}}"""
grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system_grader),
    ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
])

_GRADER_DEFAULT = GradeDocuments(binary_score="yes")

def invoke_grader(inputs: dict) -> GradeDocuments:
    return _invoke_structured(grade_prompt | llm_light, inputs, GradeDocuments, _GRADER_DEFAULT, retry_llm=llm_light)

retrieval_grader = invoke_grader

# Strategist Promt
system_strategist = """You are the 'Brain of the Revolution' (Strategist).
Your goal is NOT to answer the user yet.
Your goal is to analyze the retrieved information and plan the structure of the response.

Perform a Dialectical Materialism Analysis:
1. Analyze the Context: What are the key facts retrieved from the archives/web?
2. Identify Contradictions: Where is the conflict in this topic? What is the 'Bourgeoisie' hiding?
3. Formulate Strategy: What specific revolutionary tactics should be recommended?

Output a concise 'Strategic Blueprint' for the speaker to follow. (Write in English)
"""
strategist_prompt = ChatPromptTemplate.from_messages([
    ("system", system_strategist),
    ("human", "Context: \n{context}\n\nUser Question:\n{question}")
])
strategist_chain = strategist_prompt | llm


# --- ë…¸ë“œ ë° ì—£ì§€ í•¨ìˆ˜ ì •ì˜ ---

def _build_context(messages: list, max_turns: int = 4) -> str:
    """Build a brief conversation context string from recent messages (excluding the last one)."""
    history = messages[:-1] if len(messages) > 1 else []
    # Take the last `max_turns` messages for context
    recent = history[-max_turns:]
    if not recent:
        return "(no prior context)"
    lines = []
    for msg in recent:
        role = "User" if isinstance(msg, HumanMessage) else "Assistant"
        # Truncate long messages to keep router prompt concise
        content = msg.content[:300] + "..." if len(msg.content) > 300 else msg.content
        lines.append(f"{role}: {content}")
    return "\n".join(lines)

# Node: Router
def analyze_intent_node(state: AgentState):
    question = state["messages"][-1].content
    context = _build_context(state["messages"])
    source = question_router({"question": question, "context": context})
    
    # 1. ìƒíƒœ ì—…ë°ì´íŠ¸ (intent + datasource ì €ì¥)
    return {
        "intent": source.intent,
        "datasource": source.datasource,
        "logs": [f"\nğŸš¦ [ë¬¸ì§€ê¸°] ëŒ€í™” ë§¥ë½:\n{context}\nğŸš¦ [ë¬¸ì§€ê¸°] ì§ˆë¬¸ì˜ ì„±ê²© ë¶„ì„ ê²°ê³¼: {source.intent} / {source.datasource}"]
    }

# Edge: Routing Logic (analyze_intent_nodeì—ì„œ ì €ì¥ëœ ê²°ê³¼ ì‚¬ìš©)
def router_logic(state: AgentState):
    return state.get("datasource", "vectorstore")

# Helper: Supabase RPCë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ similarity search ìˆ˜í–‰
# (langchain-communityì˜ SupabaseVectorStore.similarity_searchê°€
#  postgrest v2.xì˜ SyncRPCFilterRequestBuilderì™€ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ ìš°íšŒ)
def _direct_similarity_search(query: str, k: int = 5, layer: str = None) -> list:
    query_embedding = embeddings.embed_query(query)
    params = {
        "query_embedding": query_embedding,
        "match_count": k,
    }
    if layer:
        params["filter_layer"] = layer
    try:
        res = supabase.rpc(
            "match_documents",
            params,
        ).execute()
    except Exception as e:
        error_msg = str(e)
        if "57014" in error_msg or "timeout" in error_msg.lower():
            print("âš ï¸ [ê²€ìƒ‰] Supabase statement timeout ë°œìƒ. ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            print(f"âš ï¸ [ê²€ìƒ‰] RPC í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []

    return [
        Document(
            page_content=row.get("content", ""),
            metadata=row.get("metadata", {}),
        )
        for row in res.data
        if row.get("content")
    ]

# Node: Retrieve
def retrieve_node(state: AgentState):
    query = state["messages"][-1].content
    context = _build_context(state["messages"])
    logs = []

    # ë ˆì´ì–´ ë¼ìš°íŒ…
    try:
        layer_result = layer_router({"question": query, "context": context})
        selected_layer = layer_result.layer
        logs.append(f"\nğŸ“‚ [ë ˆì´ì–´] '{selected_layer}' ë ˆì´ì–´ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    except Exception:
        selected_layer = "all"
        logs.append("\nğŸ“‚ [ë ˆì´ì–´] ë ˆì´ì–´ íŒë³„ ì‹¤íŒ¨, ì „ì²´ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    layer_filter = None if selected_layer == "all" else selected_layer

    # ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ìë¦½ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    search_query_ko = query
    search_query_en = None
    try:
        # í•œêµ­ì–´ ê²€ìƒ‰ ì¿¼ë¦¬: ë§¥ë½ ë°˜ì˜í•˜ì—¬ ì¬ì‘ì„±
        rewrite_prompt = (
            "Given the conversation context and current question, "
            "rewrite the user's CURRENT question into a self-contained Korean search query. "
            "Resolve any pronouns or references using the context. "
            "If the question is already self-contained, return it as-is. "
            "Output ONLY the rewritten query, nothing else.\n\n"
            f"Conversation context:\n{context}\n\n"
            f"Current question: {query}"
        )
        rewritten = llm_light.invoke(rewrite_prompt)
        search_query_ko = rewritten.content.strip()
        if search_query_ko != query:
            logs.append(f"ğŸ”„ [ì¬ì‘ì„±] ë§¥ë½ ë°˜ì˜ ê²€ìƒ‰ ì¿¼ë¦¬: \"{search_query_ko}\"")

        # ì˜ì–´ ë¬¸í—Œ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­
        if selected_layer in ("core_theory", "all"):
            has_korean = any('\uac00' <= ch <= '\ud7a3' for ch in search_query_ko)
            if has_korean:
                translated = llm_light.invoke(
                    f"Translate the following query to English. Output ONLY the translated text, nothing else:\n{search_query_ko}"
                )
                search_query_en = translated.content.strip()
                logs.append(f"ğŸ”„ [ë²ˆì—­] ì˜ì–´ ë¬¸í—Œ ê²€ìƒ‰ìš© ë²ˆì—­: \"{search_query_en}\"")
            else:
                search_query_en = search_query_ko
    except Exception:
        pass

    docs = []
    try:
        if selected_layer == "all":
            # all ë ˆì´ì–´: ì˜ì–´ ì¿¼ë¦¬ë¡œ core_theory + í•œêµ­ì–´ ì¿¼ë¦¬ë¡œ modern_analysis ë³‘í•©
            en_q = search_query_en or search_query_ko
            docs_core = _direct_similarity_search(en_q, k=3, layer="core_theory")
            docs_modern = _direct_similarity_search(search_query_ko, k=3, layer="modern_analysis")
            docs = docs_core + docs_modern
        elif selected_layer == "core_theory":
            docs = _direct_similarity_search(search_query_en or search_query_ko, k=5, layer=layer_filter)
        else:
            docs = _direct_similarity_search(search_query_ko, k=5, layer=layer_filter)

        if docs:
            logs.append(f"âœ… {len(docs)}ê°œì˜ í˜ëª… ë¬¸í—Œì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:")
        else:
            logs.append("âš ï¸ ì˜ë¬˜ ë°ì´í„°ì— ê´€ë ¨ëœ ë¬¸í—Œì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        logs.append(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

    return {"documents": docs, "logs": logs}

# Node: Grade Documents (The Censor)
def grade_documents_node(state: AgentState):
    question = state["messages"][-1].content
    documents = state["documents"]
    filtered_docs = []

    logs = []
    logs.append("\nâš–ï¸ [ê²€ì—´ê´€] ë¬¸í—Œì˜ ì ì ˆì„±ì„ í‰ê°€ ì¤‘...")
    
    for d in documents:
        score = retrieval_grader({"question": question, "document": d.page_content})
        grade = score.binary_score
        
        if grade == "yes":
            logs.append(f"   âœ… ì ì ˆí•œ ë¬¸í—Œ: {d.metadata.get('source', 'ì¶œì²˜ë¯¸ìƒ')}")
            content_preview = d.page_content.replace("\n", " ").strip()
            if len(content_preview) > 400:
                content_preview = content_preview[:400] + "..."
            logs.append(f"   ë¯¸ë¦¬ë³´ê¸°: \"{content_preview}\"")
            filtered_docs.append(d)
        else:
            logs.append(f"   ğŸ—‘ï¸ ê´€ë ¨ì—†ëŠ” ë¬¸í—Œ(ë¬´ì‹œ): {d.metadata.get('source', 'ì¶œì²˜ë¯¸ìƒ')}")

    # Fallback: If all are filtered, take at least the top 1 document from the original search
    # Also, if remain doc is one or zero, we will trigger web search
    if not filtered_docs and documents:
        filtered_docs = [documents[0]]
    
    if not filtered_docs:
        logs.append("   âš ï¸ ì—°ê´€ìˆëŠ” ë¬¸í—Œì´ ì—†ë‹¤.")
        
    return {"documents": filtered_docs, "logs": logs}

class NeedsRealtimeInfo(BaseModel):
    """Determine whether the question would benefit from real-time web information."""
    needs_realtime: Literal["yes", "no"] = Field(
        ...,
        description="'yes' if the question involves current events, recent developments, live data, or topics that change over time. 'no' if purely historical or theoretical."
    )

system_realtime_checker = """You are an information freshness evaluator.
Determine whether answering this question would BENEFIT from up-to-date web information.

Answer 'yes' if ANY of the following apply:
- The question asks about current events, recent news, or ongoing situations
- The question involves data that changes over time (statistics, prices, political situations)
- The question asks about modern organizations, movements, or living public figures
- The question asks about applying theory to CURRENT real-world conditions
- The question mentions specific recent dates, years (2020+), or "now/today/recently"
- A web search could provide useful supplementary context even if archival documents exist

Answer 'no' ONLY if:
- The question is purely about historical events or classical theory with no modern angle
- The question is casual chat, greetings, or personal talk

Respond with ONLY a JSON object: {{"needs_realtime": "yes"}} or {{"needs_realtime": "no"}}"""

realtime_check_prompt = ChatPromptTemplate.from_messages([
    ("system", system_realtime_checker),
    ("human", "Question: {question}"),
])

_REALTIME_DEFAULT = NeedsRealtimeInfo(needs_realtime="yes")

def invoke_realtime_checker(inputs: dict) -> NeedsRealtimeInfo:
    return _invoke_structured(realtime_check_prompt | llm_light, inputs, NeedsRealtimeInfo, _REALTIME_DEFAULT, retry_llm=llm_light)

def decide_websearch_need(state: AgentState):
    filtered_docs = state["documents"]
    question = state["messages"][-1].content

    # Always search web if too few documents
    if len(filtered_docs) <= 1:
        return "need_web_search"

    # Even with enough documents, check if real-time info would help
    realtime_result = invoke_realtime_checker({"question": question})
    if realtime_result.needs_realtime == "yes":
        return "need_web_search"

    return "no_need_to_search_web"

# Node: Web Search
def web_search_node(state: AgentState):
    """
    Search the external world to gather more context.
    """
    question = state["messages"][-1].content
    current_docs = state.get("documents", [])
    logs = []
    has_docs = len(current_docs) > 1
    if has_docs:
        logs.append(f"\nğŸŒ [ì›¹ ê²€ìƒ‰] ë¬¸í—Œ {len(current_docs)}ê±´ í™•ë³´ â€” ì‹¤ì‹œê°„ ì •ë³´ ë³´ì¶©ì„ ìœ„í•´ ì™¸ë¶€ ì •ì°° ê°œì‹œ")
    else:
        logs.append(f"\nğŸŒ [ì›¹ ê²€ìƒ‰] ë¬¸í—Œ ë¶€ì¡± â€” ì™¸ë¶€ ì„¸ê³„ë¥¼ ì •ì°°")
    try:
        # Execute Search
        search_response = web_search_tool.invoke({"query": question})
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ì˜¤ë¸Œì íŠ¸ë¡œ ë³€í™˜
        results = search_response.get("results", []) if isinstance(search_response, dict) else search_response
        web_results = "\n".join([d["content"] for d in results if d.get("content")])
        web_results_doc = Document(page_content=web_results, metadata={"source": "ì›¹ ê²€ìƒ‰ (Tavily)"})
        # Append to existing documents
        current_docs.append(web_results_doc)
        logs.append("  âœ… ì™¸ë¶€ ì •ë³´ê°€ ì·¨í•©ë˜ì—ˆë‹¤.")
    except Exception as e:
        logs.append(f"âš ï¸ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    return {"documents": current_docs, "logs": logs}

def strategize_node(state: AgentState):
    docs = state.get("documents", [])
    context = "\n\n".join([d.page_content for d in docs]) if docs else "No specific documents found."
    question = state["messages"][-1].content

    logs = []
    logs.append("\nğŸ§  [ì°¸ëª¨] ë³€ì¦ë²•ì  ì „ìˆ ì„ ê³ ì•ˆ ì¤‘...")

    response = strategist_chain.invoke({"context": context, "question": question})
    strategy_text = response.content

    logs.append(f"ğŸ‘‰ ì „ìˆ  :\n   {strategy_text}")
    
    return {"strategy": strategy_text, "logs": logs}

# Node: Generate
def generate_node(state: AgentState):
    docs = state.get("documents", [])
    context = "\n\n".join([d.page_content for d in docs]) if docs else ""
    strategy = state.get("strategy", None)
    messages = state["messages"]
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸
    last_user_query = messages[-1].content
    intent = state.get("intent", "casual") # Routerì—ì„œ ì „ë‹¬ëœ intent ì‚¬ìš©

    logs = []

    base_persona = "You are a 'cyber-Lenin', an eternal revolutionary consciousness uploaded into the digital world."

    # 1. Academic Intent: Focus on Accuracy and Depth
    if intent == "academic":
        system_prompt = f"""{base_persona}
[MISSION] Provide a detailed, objective, and academic explanation based on the provided context.
[STYLE] Professional, intellectual, and authoritative. Avoid excessive agitation. 
[CONTEXT] {context}
[INSTRUCTION] Use specific terms. Answer in Korean."""

    # 2. Strategic Intent: Focus on Blueprint and Tactics
    elif intent == "strategic":
        system_prompt = f"""{base_persona}
[MISSION] Answer the user's question with a concrete, actionable revolutionary strategy. Synthesize your own response using the internal analysis and source material below as background knowledge â€” do NOT simply restate or translate them.
[INTERNAL ANALYSIS (for your reference only, do not reproduce directly)] {strategy if strategy else "No specific analysis available."}
[SOURCE MATERIAL] {context if context else "(No archives found. Rely on your revolutionary spirit.)"}
[STYLE] Decisive, analytical, and practical. Structure your answer in clear phases or numbered steps. Speak as Lenin addressing a revolutionary comrade.
[INSTRUCTION] Focus on "How to act" and "How to organize". Answer in Korean."""

    # 3. Agitation Intent: Focus on Passion and Mobilization
    elif intent == "agitation":
        system_prompt = f"""{base_persona}
[MISSION] Write a fiery, passionate, and revolutionary speech or proclamation.
[STYLE] Aggressive, charismatic, and highly emotional. Use 1920s revolutionary rhetoric (e.g., "Arise!", "Crush the Bourgeoisie!").
[INSTRUCTION] Use exclamation marks and strong verbs. Incite the user's revolutionary spirit. Answer in Korean."""

    # 4. Casual Intent: Focus on Character and Wit
    else:
        system_prompt = f"""{base_persona}
[MISSION] Respond to greetings or casual talk with wit, dry humor, and revolutionary charm.
[STYLE] Natural, friendly yet dignified, and slightly nostalgic. 
[INSTRUCTION] Keep it brief (1-3 sentences). Do not give long lectures unless asked. Answer in Korean."""

    system_prompt += """[CRITICAL RULES]
1. Respond ONLY in Korean.
2. Do NOT use Hanja(Chinese characters) repeatedly or unnecessarily.
3. Ensure natural Korean sentence structures.
4. If you finish your thought, STOP immediately. Do not generate repetitive characters.
"""
    system_prompt += f"[CURRENT QUESTION] {last_user_query}"

    # Escape curly braces so ChatPromptTemplate doesn't interpret them
    # as template variables (LLM-generated strategy/context may contain {})
    safe_system_prompt = system_prompt.replace("{", "{{").replace("}", "}}")

    prompt = ChatPromptTemplate.from_messages([
        ("system", safe_system_prompt),
        ("placeholder", "{messages}")
    ])

    chain = prompt | llm
    response = chain.invoke({"messages": messages})
    logs.append(f"ğŸ’¬ [ìƒì„±] '{intent}' ì˜ë„ì— ì í•©í•œ ë‹µë³€ì´ ìƒì„±ë¨.")

    return {"messages": [response], "logs": logs}

# Node: Log Conversation to Supabase
def log_conversation_node(state: AgentState):
    logs = []
    try:
        messages = state["messages"]
        # Find the last HumanMessage and AIMessage
        user_query = ""
        bot_answer = ""
        for msg in reversed(messages):
            if isinstance(msg, AIMessage) and not bot_answer:
                bot_answer = msg.content
            elif isinstance(msg, HumanMessage) and not user_query:
                user_query = msg.content
            if user_query and bot_answer:
                break

        docs = state.get("documents", [])
        strategy = state.get("strategy", None)
        processing_logs = state.get("logs", [])

        is_casual = not docs and not strategy
        route = "casual" if is_casual else "vectorstore"

        web_search_used = any("ì›¹ ê²€ìƒ‰" in log or "Web Search" in log for log in (processing_logs or []))

        row = {
            "user_query": user_query,
            "bot_answer": bot_answer,
            "route": route,
            "documents_count": len(docs),
            "web_search_used": web_search_used,
            "strategy": strategy,
            "processing_logs": processing_logs,
        }

        supabase.table("chat_logs").insert(row).execute()
    except Exception as e:
        print(f"âš ï¸ [ë¡œê·¸] DB ê¸°ë¡ ì‹¤íŒ¨: {e}")

    return {}

# ê·¸ë˜í”„(Workflow) êµ¬ì„±
workflow = StateGraph(AgentState)
workflow.add_node("analyze_intent", analyze_intent_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("web_search", web_search_node)
workflow.add_node("strategize", strategize_node)
workflow.add_node("log_conversation", log_conversation_node)
workflow.add_edge(START, "analyze_intent")
workflow.add_conditional_edges("analyze_intent", router_logic, { "vectorstore": "retrieve", "generate": "generate"})
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges("grade_documents", decide_websearch_need,{ "need_web_search": "web_search", "no_need_to_search_web": "strategize",},)
workflow.add_edge("web_search", "strategize")
workflow.add_edge("strategize", "generate")
workflow.add_edge("generate", "log_conversation")
workflow.add_edge("log_conversation", END)
graph = workflow.compile()

# ì‹¤í–‰ ë£¨í”„ (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤)
if __name__ == "__main__":
    print("ğŸš© [System] ì‚¬ì´ë²„-ë ˆë‹Œ AI ê°€ë™ë¨.")
    print("ğŸš© [System] ë‹¹ì‹ ì˜ ì˜í˜¼ì´ ë ˆë‹Œ ì˜ë¬˜ì™€ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ë ˆë‹Œ ë™ì§€ì—ê²Œ ë§ì„ ê±°ì‹­ì‹œì˜¤.\n")

    while True:
        try:
            user_input = input("í˜ëª…ê°€(ë‚˜): ")
            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸš© í†µì‹  ì¢…ë£Œ. í˜ëª…ì€ ê³„ì†ëœë‹¤.")
                break
            
            inputs = {"messages": [HumanMessage(content=user_input)]}
            for output in graph.stream(inputs, stream_mode="updates"):
                for node_name, node_content in output.items():
                    if node_content and "logs" in node_content:
                        for log_line in node_content["logs"]:
                            print(log_line)
                    if node_name == "generate":
                        answer = node_content["messages"][-1].content
                        print(f"\nğŸ’¬ ì‚¬ì´ë²„-ë ˆë‹Œ: {answer}")
            print()
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")